<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Accelerating first-order methods | Sketchy Polytopes</title><meta name="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="name" content="Umayr Hassan"><meta itemprop="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="image" content="http://localhost:4000/assets/images/ChebyshevTSpiral.gif"><meta property="og:url" content="http://localhost:4000/2014/07/21/accelerating-first-order-methods/"><meta property="og:type" content="website"><meta property="og:title" content="Accelerating first-order methods | Sketchy Polytopes"><meta property="og:site_name" content="Sketchy Polytopes"><meta property="og:description" content="Algorithms, optimization; systems, data, and people "><meta property="og:image" content="http://localhost:4000/assets/images/ChebyshevTSpiral.gif"><meta name="twitter:url" content="http://localhost:4000/2014/07/21/accelerating-first-order-methods/"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Accelerating first-order methods | Sketchy Polytopes"><meta name="twitter:site" content="Sketchy Polytopes"><meta name="twitter:description" content="Algorithms, optimization; systems, data, and people "><meta property="twitter:image" content="http://localhost:4000/assets/images/ChebyshevTSpiral.gif"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="Sketchy Polytopes" href="/feed.xml"><link rel="canonical" href="/2014/07/21/accelerating-first-order-methods/"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body id="accelerating-first-order-methods" class="post-layout"><header class="header"> <a class="header__title" href="http://localhost:4000/">Sketchy Polytopes</a><nav><ul class="header__list"><li><a href="/">Posts</a></li><li><a href="/notes">Notes</a></li><li><span class="popup__open">Contact</span></li></ul></nav></header><main class="💈"><div class="post"><article itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding--double"><div class="grid-small"><h2 itemprop="name headline">Accelerating first-order methods</h2><time class="post__date" datetime="2014-07-21T00:00:00-07:00" itemprop="datePublished">21 Jul 2014</time></div></div><div class="post__img"><div><figure class="absolute-bg" style="background-image: url('/assets/images/ChebyshevTSpiral.gif');"></figure></div></div><div class="post__content section-padding"><div class="grid"><div id="markdown" itemprop="articleBody"><p>The lower bound on the oracle complexity of continuously differentiable, $\beta$-smooth convex function is $O(\frac{1}{\sqrt{\epsilon}})$ [Theorem 2.1.6, Nesterov04; Theorem 3.8, Bubeck14; Nesterov08]. General first-order gradient descent does not achieve this - e.g. L1-Prox, or ISTA, achieves $O(\frac{1}{\epsilon})$. Nesterov, in a series of papers [Nesterov83, Nesterov88, Nesterov07], proposed techniques to improve the convergence rate for smooth functions to $O(\frac{1}{t^2})$. In this post, we discuss Nesterov acceleration.</p><h2 id="accelerated-gradient-descentalpha-strongly-convexbeta-smooth">Accelerated gradient descent: $\alpha$-strongly convex, $\beta$-smooth</h2><p><em>Algorithm</em>: Given a starting point x1 = y1,  then for t ≥ 1</p><script type="math/tex; mode=display">y_{k+1} = x_k - \frac{1}{\beta}\nabla{f(x_k)} \\ x_{k+1} = (1 + \frac{\sqrt{Q}-1}{\sqrt{Q}+1})y_{k+1}-(\frac{\sqrt{Q}-1}{\sqrt{Q}+1})y_{k}</script><p><em>Theorem</em>: For α-strongly convex, β-smooth functions with condition number Q = β/α, Nesterov’s accelerated gradient descent satisfies [Theorem 3.11, Bubeck14], which comes close to the lower bound [Theorem 2.1.13, Nesterov04]:</p><script type="math/tex; mode=display">f(x_k) - f(x^{\ast}) \le \frac{\alpha + \beta}{2}||x_1 - x^{\ast}||^2 \exp{(-\frac{k - 1}{\sqrt{Q}})}</script><p><em>Proof sketch</em>:</p><ol><li>Following [Bubeck14], one can define an α-strongly convex quadratic approximation to f(x) that improves with each iteration:</li></ol><script type="math/tex; mode=display">\Phi_1(x) = f(x_1) + \frac{\alpha}{2}||x - x_1||^2 \\ \Phi_{k+1}(x) = (1 - \frac{1}{\sqrt{Q}}) \Phi_k(x) + \frac{1}{\sqrt{Q}}(f(x_k) + \nabla{f(x_k)}(x - x_k) + \frac{\alpha}{2}||x_k - x||^2)</script><p>Then, using the definition of α-strong convexity, one can show that</p><script type="math/tex; mode=display">\Phi_{k+1}(x) \le f(x) + (1 - \frac{1}{\sqrt{Q}})^k (\Phi_1(x) - f(x))</script><ol><li>Now, assuming the following inequality is true,</li></ol><script type="math/tex; mode=display">f(y_k) \le \min_x{\Phi_k(x)}</script><ol><li>Then one can argue that</li></ol><script type="math/tex; mode=display">f(y_k) - f(x^{\ast}) \le \Phi_k(x^{\ast}) - f(x^{\ast}) \\ \le (1 - \frac{1}{\sqrt{Q}})^{t-1}(\Phi_1(x^{\ast}) - f(x^{\ast})) \\ \le \frac{\alpha + \beta}{2}||x_1 - x^{\ast}||^2 (1 - \frac{1}{\sqrt{Q}})^{t-1}</script><h3 id="accelerated-gradient-descentbeta-smooth">Accelerated gradient descent: $\beta$-smooth</h3><p><em>Algorithm</em>: Given a starting point x1 = y1,  then for t ≥ 1</p><script type="math/tex; mode=display">\lambda_0 = 0, \lambda_k = \frac{1+\sqrt{1 + 4\lambda_{k-1}^2}}{2}, \gamma_k = \frac{1 - \lambda_k}{\lambda_{k+1}} \\ y_{k+1} = x_k - \frac{1}{\beta}\nabla{f(x_k)} \\ x_{k+1} = (1 - \gamma_k)y_{k+1} + \gamma_k y_k</script><p><em>Theorem</em>: For non-strictly convex functions, where α = 0, gradient descent satisfies [Theorem 3.12, Bubeck14], which is within a constant factor of the lower bound [Theorem 2.1.7 in Nesterov04]:</p><script type="math/tex; mode=display">f(x_k) - f(x^{\ast}) \le \frac{2\beta||x_1 - x^{\ast}||^2}{k^2}</script><p><em>Proof sketch</em>:</p><ol><li>In a gradient descent scheme for β-smooth functions [Lemma 3.4, Bubeck14]</li></ol><script type="math/tex; mode=display">f(y_{k+1}) - f(y_k) \le \nabla{f(x_k)(x_k - y_k) - \frac{1}{2\beta}}||\nabla{f(x_k)}||^2 \\ = \beta (x_k - y_{k+1})(x_k - y_k) - \frac{\beta}{2}||x_k - y_{k+1}||^2 \\ f(y_{k+1}) - f(x^{\ast}) \le \beta (x_k - y_{k+1})(x_k - y_k) - \frac{\beta}{2}||x_k - y_{k+1}||^2</script><ol><li> Let δs = f(ys) - f(x^{\ast}), then scaling the first inequality by (λs - 1) and adding the result to second</li></ol><script type="math/tex; mode=display">\lambda_k \delta_{k+1} - (\lambda_k - 1) \delta_k \\ \le \beta (x_k - y_{k+1})(\lambda_k x_k - (\lambda_k - 1)y_k - x^{\ast}) - \frac{\beta}{2} \lambda_k||x_k - y_{k+1}||^2</script><ol><li>Scaling by λs and after some manipulation</li></ol><script type="math/tex; mode=display">\lambda_k^2 \delta_{k+1} - \lambda_{k-1}^2 \delta_k \\ \le \frac{\beta}{2}(||\lambda_k x_k - (\lambda_k - 1)y_k - x^{\ast 2}||^2 -||\lambda_k y_{k+1} - (\lambda_k - 1)y_k - x^{\ast 2}||^2) \\ = \frac{\beta}{2}(||u_s||^2 - ||u_{k+1}||^2) \ where\ u_k=\lambda_k x_k- (\lambda_k - 1)y_k - x^{\ast}</script><ol><li>Summing these inequalities from k = 1 to k = t - 1, yields</li></ol><script type="math/tex; mode=display">\delta_t \le \frac{\beta}{2\lambda_{t-1}^2}||u_1||^2, \ and\ \lambda_{t-1} \ge \frac{t}{2}</script><h2 id="interpretation-chebychev-polynomials">Interpretation: Chebychev polynomials</h2><p>Moritz Hardt elegantly describes an interpretation of accelerated gradient descent based on function interpolation [Hardt13]. Gradient descent can be seen as approximating a function using a degree-k polynomial. For strongly convex functions, the derivative can be expressed as a linear combination of previous steps i.e.</p><script type="math/tex; mode=display">x_{k+1} = x_k + \eta_k \nabla{f(x_k)} \\ = A^{T}x + b</script><p>Assuming that ∑λs = 1,</p><script type="math/tex; mode=display">x_{k+1} - x^{\ast} \\ = \sum_{s=1}^{k}(\lambda_s (x_s - x^{\ast})) \\ = \sum_{s=1}^{k}(\lambda_s A^s) (x_1 - x^{\ast}) \\ = P_{k}(A)(x_1 - x^{\ast}) \\ where\ P_{k}(A) = \sum_{s=1}^{k}(\lambda_s A^s)</script><p>Thus given that</p><script type="math/tex; mode=display">||x_{k+1} - x^{\ast}|| = ||P_{k}(\mu)||_A\ ||x_1 - x^{\ast}||</script><p>$P(\mu)$ needs to be chosen over all eigenvalues of A so that (1) $P_{k}(\mu)$ = 1, for $\mu$ = 0, and (2) its maximum norm is minimized. This is hard since it requires knowing all eigenvalues but can be achieved, under certain conditions, if the domain is assumed to be continuous [Kelner09]. In short, a scaled and shifted <a href="http://en.wikipedia.org/wiki/Chebyshev_polynomials">Chebychev polynomial</a> is the unique polynomial that minimizes this approximation error. For Chebychev polynomial, the maximum error is</p><script type="math/tex; mode=display">\max{(||P_{k}(\mu)||)} = (\frac{\sqrt{Q}-1}{\sqrt{Q} + 1})^k</script><p>Moreover, since Chebychev polynomials can be expressed recursively, required only the two previously calculated polynomials, the gradient descent update only depends on last two values.</p><p>This method seems to be well known in Numerical Analysis, where it is used to speed up iterative linear solvers [chapter 4, Kincaid02; section 7.4,  Saad11]. It also seems to predate Nesterov’s work [Manteuffel77].</p><h2 id="extension-bregman-divergence">Extension: Bregman Divergence</h2><p>Paul Tseng uses <a href="http://mark.reid.name/blog/meet-the-bregman-divergences.html">Bregman Divergences</a> to give a unified framework for Nesterov’s methods across different classes of problems [Tseng08].</p><p>See also [Taboulle10, Candes11, Hao11, Gordon12] for a good overview of first-order acceleration methods. Next post discusses techniques that utilize accelerated gradient descent to solve regularization problems [Nesterov07, Beck09, Becker09].</p><h2 id="references">References</h2><ul><li><a href="http://mechroom.technion.ac.il/~becka/papers/71654.pdf">Beck09</a> A. Beck, M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal of Imaging Sciences, 2009</li><li><a href="http://statweb.stanford.edu/~candes/nesta/NESTA.pdf">Becker09</a> S. Becker, J. Bobin, E. Candes. NESTA: A Fast and Accurate First-Order Method for Sparse Recovery. Technical Report, Caltech, 2009</li><li><a href="http://www.princeton.edu/~sbubeck/Bubeck14.pdf">Bubeck14</a> S. Bubeck, Theory of Convex Optimization for Machine Learning</li><li><a href="http://statweb.stanford.edu/~candes/math301/Lectures/fast_proximal_methods.pdf">Candes11</a> E. Candes. Math 301, Lectures Notes.</li><li><a href="http://statweb.stanford.edu/~candes/math301/Lectures/acc_nesterov.pdf">Chen11</a> H. Chen, X. Ming. Accelerating Nesterov’s Method for Strongly Convex Functions. Course presentation, 2011.</li><li><a href="http://mrtz.org/blog/the-zen-of-gradient-descent/">Hardt13</a> M. Hardt. The Zen of Gradient Descent. Blog, 2013.</li><li><a href="http://www.cfm.brown.edu/people/gk/chap7/node21.html">Karniadakis00</a> G. E. Karniadakis. R. M. Kirby. Parallel Scientific Computing in C++ and MPI. 2000</li><li><a href="http://ocw.mit.edu/courses/mathematics/18-409-topics-in-theoretical-computer-science-an-algorithmists-toolkit-fall-2009/lecture-notes/MIT18_409F09_scribe22.pdf">Kelner09</a> J. Kelner. Lecture Notes. MIT 18.409, Lecture 22, 2009</li><li><a href="https://bookstore.ams.org/amstext-2">Kincaid02</a> D. R. Kincaid, E. W. Cheney. Numerical Analysis: Mathematics of Scientific Computing. AMS, 2002</li><li>[Nesterov83] Y. Nesterov. A method for solving a convex programming problem with convergence rate O(1/k2). Dokaldy AN SSR, 1983</li><li>[Nesterov88] Y. Nesterov. On an approach to the construction of optimal methods of minimization of smooth convex functions. Ekonom. i. Mat. Metody, 1988</li><li>[Nesterov04] Y. Nesterov. Introductory Lectures On Convex Programming: A Basic Course. Kluwer Academic Publishers, 2004</li><li><a href="http://www.optimization-online.org/DB_FILE/2007/09/1784.pdf">Nesterov07</a> Y. Nesterov. Gradient methods for minimizing composite objective function. Report, CORE, 2007</li><li><a href="http://galton.uchicago.edu/~lekheng/courses/31060s13/nesterov.pdf">Nesterov08</a> Y. Nesterov. How to advance in Structural Convex Optimization. Optima, 2008</li><li><a href="http://link.springer.com/article/10.1007%2FBF01389971#page-1">Manteuffel77</a> T. A. Manteuffel. The Tchebyshev iteration for nonsymmetric linear systems. Numer. Math, 1977.</li><li><a href="http://www-users.cs.umn.edu/~saad/eig_book_2ndEd.pdf">Saad11</a> Y. Saad. Numerical Methods for Large Eigenvalue Problems. SIAM, 2011</li><li><a href="https://www.ipam.ucla.edu/publications/optut/optut_9300.pdf">Taboulle10</a>. M. Taboulle. First-Order Methods for Optimization. IPAM Optimization Tutorials, 2010</li><li><a href="https://www.cs.cmu.edu/~ggordon/10725-F12/slides/09-acceleration.pdf">Gordon12</a> G. Gordon, R. Tibshirani. Course Notes, 10-725 Optimization, Fall 2012</li><li><a href="http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf">Tseng08</a> P. Tseng. On Accelerated Proximal Gradient Algorithms for Convex-Concave Optimization. SIAM Journal of Optimization, 2008</li></ul><h2 id="credit">Credit</h2><p>Main image from Wolfram’s <a href="http://mathworld.wolfram.com/ChebyshevPolynomialoftheFirstKind.html">Chebyshev Polynomial of the First Kind</a>: “A beautiful plot can be obtained by plotting $T_{n}(x)$ radially, increasing the radius for each value of $n$, and filling in the areas between the curves (Trott 1999, pp. 10 and 84).”</p></div><ul class="post__social"><li><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-facebook"></i></a></li><li><a href="https://twitter.com/intent/tweet?&text=Accelerating first-order methods+http://localhost:4000/2014/07/21/accelerating-first-order-methods/+by+Umayr Hassan" target="_blank"><i class="fa fa-twitter"></i></a></li><li><a href="https://plus.google.com/share?url=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-google-plus"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&source=Accelerating first-order methods&summary=&url=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://www.stumbleupon.com/badge/?url=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-stumbleupon"></i></a></li><li><a href="https://www.reddit.com/submit?url=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-reddit-alien"></i></a></li><li><a href="https://www.tumblr.com/share/link?url=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-tumblr"></i></a></li><li><a href="https://www.pinterest.com/pin/create/link/?description=&media=http://localhost:4000/assets/images/ChebyshevTSpiral.gif&url=http://localhost:4000/2014/07/21/accelerating-first-order-methods/" target="_blank"><i class="fa fa-pinterest"></i></a></li></ul></div></div><div class="section-padding--none"><div class="grid"><hr class="sep"/></div></div><div class="section-padding"><div class="grid-small"> <span class="post__author">Posted by <a href="http://umayrh.github.io" title="More By Umayr Hassan">Umayr Hassan</a></span><p class="post__bio"></p></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="http://localhost:4000/2016/12/23/a-fearful-sphere-whose-center-is-everywhere/"><figure class="related__img"> <img src="/assets/images/screen-shot-2016-12-16-at-10-28-14-pm.png" alt="A fearful sphere, whose center is everywhere"/></figure><div><h2 class="related__text">A fearful sphere, whose center is everywhere</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/10/06/obituaries-harold-kuhn-1925-2014/"><div><h2 class="related__text">Obituaries: Harold Kuhn (1925–2014)</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><figure class="related__img"> <img src="/assets/images/agd.png" alt="Accelerated first-order methods for regularization"/></figure><div><h2 class="related__text">Accelerated first-order methods for regularization</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get a weekly email of posts I’ve added to the site.</p><form method="POST" action="&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__tags"><li><a class="footer__link" href="/tag/mathematics">Mathematics</a></li><li><a class="footer__link" href="/tag/algorithms">Algorithms</a></li><li><a class="footer__link" href="/tag/convex-optimization">Convex Optimization</a></li><li><a class="footer__link" href="/tag/java">Java</a></li><li><a class="footer__link" href="/tag/obit">Obit</a></li></ul><ul class="footer__social"><li><a href="https://www.linkedin.com/in/umayr/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://github.com/umayrh" target="_blank"><i class="fa fa-github"></i></a></li><li><a href="https://gitlab.com/umayrh" target="_blank"><i class="fa fa-gitlab"></i></a></li><li><a href="https://umayrh.wordpress.com" target="_blank"><i class="fa fa-wordpress"></i></a></li></ul></div></div></footer><section class="contact popup"><div class="popup__close"><div class="popup__exit"></div></div><div class="contact__container popup__container"><div class="contact__img"><figure class="absolute-bg" style="background-image: url(/assets/images/form_contact.jpg);"></figure></div><div class="contact__content"><div class="contact__mast section-padding--half"><div class="grid"><h2>Contact</h2></div></div><div class="section-padding--none"><hr class="sep"/></div><div class="contact__form section-padding--half"><div class="grid-xlarge"> <form id="form" class="form" action="https://formcarry.com/s/UuffFC2ATgC" method="POST"><div class="form__subcontainer"><div> <label for="form-first-name">First Name</label> <input type="text" name="first-name" id="form-first-name" required></div><div> <label for="form-last-name">Last Name</label> <input type="text" name="last-name" id="form-last-name" required></div></div><div> <label for="form-email">E-Mail</label> <input type="email" name="email" id="form-email" required></div><div> <label for="form-message">Message</label> <textarea name="message" id="form-message" rows="3"></textarea></div><div class="form__submit"><div class="form__btn"> <input type="submit" value="Send"></div></div><p class="form__message"></p></form></div></div></div></div></section><script src="/assets/js/app.min.js"></script></body></html>
  