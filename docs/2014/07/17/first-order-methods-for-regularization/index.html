<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>First-order methods for regularization | Sketchy Polytopes</title><meta name="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="name" content="Umayr Hassan"><meta itemprop="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="image" content="http://umayrh.github.io/assets/images/first-regular.png"><meta property="og:url" content="http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/"><meta property="og:type" content="website"><meta property="og:title" content="First-order methods for regularization | Sketchy Polytopes"><meta property="og:site_name" content="Sketchy Polytopes"><meta property="og:description" content="Algorithms, optimization; systems, data, and people "><meta property="og:image" content="http://umayrh.github.io/assets/images/first-regular.png"><meta name="twitter:url" content="http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="First-order methods for regularization | Sketchy Polytopes"><meta name="twitter:site" content="Sketchy Polytopes"><meta name="twitter:description" content="Algorithms, optimization; systems, data, and people "><meta property="twitter:image" content="http://umayrh.github.io/assets/images/first-regular.png"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="Sketchy Polytopes" href="/feed.xml"><link rel="canonical" href="/2014/07/17/first-order-methods-for-regularization/"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body id="first-order-methods-for-regularization" class="post-layout"><header class="header"> <a class="header__title" href="http://umayrh.github.io/">Sketchy Polytopes</a><nav><ul class="header__list"><li><a href="/">Posts</a></li><li><a href="/notes">Notes</a></li><li><span class="popup__open">Contact</span></li></ul></nav></header><main class="üíà"><div class="post"><article itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding--double"><div class="grid-small"><h2 itemprop="name headline">First-order methods for regularization</h2><time class="post__date" datetime="2014-07-17T00:00:00+05:00" itemprop="datePublished">17 Jul 2014</time></div></div><div class="post__img"><div><figure class="absolute-bg" style="background-image: url('/assets/images/first-regular.png');"></figure></div></div><div class="post__content section-padding"><div class="grid"><div id="markdown" itemprop="articleBody"><p>General first-order methods use only the values of objective/constraints and their subgradients to optimize non-smooth (including constrained) function [5]. Such methods that are oblivious to the structure of the problem are termed <em>black-box optimization</em> techniques.</p><h2 id="why-first-order">Why first-order?</h2><p>Primarily because the computation cost, per iteration, of high-accuracy methods like interior-point generally increases non-linearly, which is prohibitively expensive for very large data-sets (&gt; 100K variables).</p><p>Though the cost per iteration of general methods is low, these methods cannot exploit the structure of the problem. This post discusses two first-order optimization techniques for regularization problems that can exploit the structure of the problem to give better convergence rate than general methods.</p><h2 id="regularization">Regularization</h2><p>Regularization problems in statistics and machine learning, or denoting problems in signal processing, take one of the following forms:</p><p><script type="math/tex">\min{(\frac{1}{2}\|Ax - b\|^2 + \lambda \|x\|)}</script> <script type="math/tex">\min{f(x)} \mbox{ s.t. } \| b - Ax \| \le \epsilon</script></p><p>where $|.|$ indicates some kind of a norm (or seminorm), e.g. $L_1$ or $L_2$ in statistics and Total Variation norm in imaging, and $\lambda$ is a real-valued regularization parameter. Optimization of such composite convex functions - often with a smooth component, $f(x)$, and a non-smooth component, $g(x)$ - can be expressed as:</p><script type="math/tex; mode=display">\min_{x \in \mathbb{R}} {f(x) + \lambda g(x)}</script><p>Regularization imposes a structure, using a specific norm, on the solution. For example, using L1 norm encourages sparsity, which often results in more noise-tolerant solutions. This was the motivation behind ridge regression and LASSO [8] in statistical estimation. Unconstrained regularization problems can be solved using (constrained) linear programming while constrained problems may be solved using second-order cone program. Both programs are variants of interior-points algorithms, and too slow for very large data sets. Various first-order methods may be used to solve regularization problem. In fact, for the specific case of regularization problems, first-order method can be shown to converge to an Œµ-solution at an optimal rate.</p><h2 id="complexity">Complexity</h2><p>The complexity of first-order methods is often measured in terms of the number of calls to a first-order oracle (section 5.1 in [5]) required to reach an $\epsilon$-solution. Conversely, the convergence rate gives the approximation error after <em>t</em> iterations. Nemirovski and Yudin‚Äôs work [5] established a lower bound for the information-based complexity of black-box optimization techniques: $O(\frac{1}{\epsilon^2})$ iterations (alternatively, an $O(\frac{1}{\sqrt{t}})$-precise solution after $t$ iterations). Furthermore, in many cases, the complexity is still dependent on the dimension, $n$, of the problem - $O(n)$, in the worst case. For very large dimensional problems, such black-box schemes would converge too slowly.</p><p>Nonetheless, there are large classes of problems - including regularization - for which the complexity is $O(\frac{1}{\epsilon})$ or, in some cases, $O(\frac{1}{\sqrt{\epsilon}})$ (theorem 3.8 in [7], section 5.1 in [9] [10, 12]). In such cases, the complexity is also either independent of or only sublinearly dependent on problem dimension, making them feasible for very large dimensional but medium-accuracy problems (such as those in machine learning).</p><p>Why not use polynomial-time interior-point methods for convex optimization?</p><p>While interior-point methods converge rapidly - $Œü(e^{Œü(t)})$ - to an Œµ-solution, the complexity of each iteration is super-linear - $O(nk)$, $k &gt; 1$. For very large dimensional problems, these methods are prohibitively expensive.</p><h2 id="proximal-algorithms">Proximal algorithms</h2><script type="math/tex; mode=display">\min_{x \in \mathbb{R}} {f(x) + \lambda g(x)}</script><p>For the composite function optimization problem, we assume that:</p><ul><li>the problem has a unique minimizer</li><li>$f(x)$ is $\beta$-smooth, convex and differentiable</li><li>$g(x)$ is convex, subdifferentiable and ‚Äúsimple‚Äù (to be clarified later)</li></ul><p>From the definition of $\beta$-smooth convex function, f(x):</p><script type="math/tex; mode=display">f(x_{k+1}) \le f(x_k)+ \nabla{f(x_k)}(x_{k+1} - x_k) + \frac{\beta}{2} \|x_{k+1} - x_k\|^2</script><p>For subgradient descent from a given a point $x_k$, we need to find the next point xk+1 such that $f(x_{k}+1)$ is minimized i.e [3].</p><script type="math/tex; mode=display">x_{k+1} = \mathrm{argmin}_{x \in \mathbb{R}} f(x_k)+ \nabla{f(x_k)}(x - x_k) + \frac{\beta}{2} \|x - x_k\|^2</script><p>For the composite function, $f(x) + \lambda g(x)$, this becomes</p><script type="math/tex; mode=display">x_{k+1} = \mathrm{argmin}_{x \in \mathbb{R}} \lambda g(x)+ \nabla{f(x_k)}(x - x_k) + \frac{\beta}{2} \|x - x_k\|^2</script><p>which can be reduced to</p><script type="math/tex; mode=display">x_{k+1} = \mathrm{argmin}_{x \in \mathbb{R}} \lambda g(x)+\frac{\beta}{2} \|x - (x_k - \frac{1}{\beta}\nabla{f(x_k)})\|^2</script><p>Despite the fact the each iteration here involves another optimization, a wide class of problems in this form can be solved with <em>proximal minimization algorithms</em> [13, 14]<em>.</em> Proximal algorithms minimizing a $\beta$-smooth function $f(x)$ can in general be represented as:</p><script type="math/tex; mode=display">x_{k+1} = prox_{\eta,f}(x_k)</script><p>where $\eta = \frac{1}{\beta}$ and the proximity operator, $prox_{\eta}f(y)$, of a scaled function $f(x)$ is defined as</p><script type="math/tex; mode=display">\mathrm{argmin}_{x \in \mathbb{R}} f(x) + \frac{1}{2\eta} \|x - y\|^2</script><p>For the composite function, $f(x) + \lambda g(x)$, this implies</p><script type="math/tex; mode=display">x_{k+1} = prox_{\lambda\eta,g(x)}(x_k - \frac{1}{2\eta\lambda}\nabla{f(x_k)})</script><p>Proximal algorithms are useful when the optimization subproblems either admit a closed-form solution or can be rapidly solved numerically. If this is the case, then $g(x)$ is considered ‚Äúsimple.‚Äù For example, $g(x)$ in regularization problems is an $L_p$ norm, where p = {1, 2, $\infty$}. Using the calculus of proximity operators, it is possible to evaluate the closed-form solutions for these norm:</p><p>$prox_{\eta,|.|_{1}}(y)= \begin{cases} y-\eta, &amp; y \ge \eta \ 0, &amp; |y| &lt; \eta \ y+\eta, &amp; y \le \eta \end{cases}$</p><p>This operation, for $L_1$ norm, is also called <em>soft thresholding</em>.</p><p>$prox_{\eta,|.|_{2}}(y)= \begin{cases} (1-\frac{\eta}{|y|_2})y, &amp; |y|_2 \ge \eta \ 0, &amp; |y|_2 &lt; \eta\end{cases}$</p><p>The operation for $L_2$ norm is also called <em>blockwise soft thresholding</em>.</p><p>How rapidly would this scheme converge for composite functions compared to subgradient descent (since $g(x)$ is non-smooth)? Since the proximal algorithm does not need to evaluate the subgradient of $g(x)$ - only the gradient of $f(x)$, which is $\beta$-smooth - the convergence rate should the same as that $f(x)$. Following Theorem 3.3 in [7], given the minimum $x^{\ast}$,</p><script type="math/tex; mode=display">(f(x_k) + \lambda g(x_k)) - (f(x^{\ast}) + \lambda g(x^{\ast})) \le O(1)\frac{\beta \|x_0 - x^{\ast}\|}{k}</script><p>which is much faster than subgradient descent‚Äôs $O(\frac{1}{\sqrt{k}})$. Improving convergence rate to $O(\frac{1}{k^2})$ [1,2] will be the topic of our next post.</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## Corrupted signal and model</span><span class="w">
 
</span><span class="n">x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="m">5</span><span class="p">,</span><span class="w"> </span><span class="m">6</span><span class="p">)</span><span class="w">
</span><span class="n">t</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="n">from</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="n">by</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">0.02</span><span class="p">,</span><span class="w"> </span><span class="n">to</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">2</span><span class="o">*</span><span class="nb">pi</span><span class="p">)</span><span class="w">
</span><span class="n">A</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">cbind</span><span class="p">(</span><span class="nf">sin</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="m">2</span><span class="o">*</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="m">3</span><span class="o">*</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="m">4</span><span class="o">*</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="m">5</span><span class="o">*</span><span class="n">t</span><span class="p">),</span><span class="w"> </span><span class="nf">sin</span><span class="p">(</span><span class="m">6</span><span class="o">*</span><span class="n">t</span><span class="p">))</span><span class="w">
</span><span class="n">e</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">-4+8</span><span class="o">*</span><span class="n">rnorm</span><span class="p">(</span><span class="nf">length</span><span class="p">(</span><span class="n">t</span><span class="p">),</span><span class="m">0</span><span class="p">,</span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="c1">#e[100:115] &lt;- 30</span><span class="w">
</span><span class="n">y</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">e</span><span class="w">
 
</span><span class="n">plot</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s1">'blue'</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"signal"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"time"</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">y</span><span class="p">)</span><span class="w">
 
</span><span class="c1">## Proximal algorithm for l1</span><span class="w">
 
</span><span class="c1">## parameters</span><span class="w">
</span><span class="n">iter</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1000</span><span class="w">
</span><span class="n">beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"F"</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w">
</span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
</span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
</span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
 
</span><span class="c1">## main loop</span><span class="w">
</span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
 </span><span class="n">gradient_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w">
 </span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x</span><span class="w">
 </span><span class="n">v</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">lambda</span><span class="w">
 </span><span class="c1"># L1 prox/shrinkage-thresholding</span><span class="w">
 </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="o">-</span><span class="w"> </span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
</span><span class="p">}</span><span class="w">
 
</span><span class="n">xx</span><span class="w">
 
</span><span class="n">lines</span><span class="p">(</span><span class="n">t</span><span class="p">,</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">xx</span><span class="p">,</span><span class="w"> </span><span class="n">col</span><span class="o">=</span><span class="s2">"red"</span><span class="p">)</span><span class="w">
 
</span><span class="n">legend</span><span class="p">(</span><span class="s2">"topright"</span><span class="p">,</span><span class="w"> </span><span class="n">legend</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"original signal"</span><span class="p">,</span><span class="w"> </span><span class="s2">"corrupted signal"</span><span class="p">,</span><span class="w"> </span><span class="s2">"recovered signal"</span><span class="p">),</span><span class="w">
 </span><span class="n">col</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="s2">"black"</span><span class="p">,</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="s2">"red"</span><span class="p">),</span><span class="w"> </span><span class="n">lwd</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w">
</span></code></pre></div></div><p><img src="http://umayrh.files.wordpress.com/2014/07/l1prox1.png?w=300" alt="Signal recovery using L1-Prox" /> Signal recovery using L1-Prox</p><h2 id="references">References</h2><ul><li><a href="http://mechroom.technion.ac.il/~becka/papers/71654.pdf">1</a> A. Beck, M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM J. Imaging Sciences, 2009</li><li><a href="http://statweb.stanford.edu/~candes/nesta/NESTA.pdf">2</a> S. Becker, J. Bobin, E. Candes. NESTA: A Fast and Accurate First-Order Method for Sparse Recovery. Technical Report, Caltech, 2009</li><li><a href="http://blogs.princeton.edu/imabandit/2013/04/11/orf523-ista-and-fista/">3</a> http://blogs.princeton.edu/imabandit/2013/04/11/orf523-ista-and-fista/</li><li><a href="https://www.ipam.ucla.edu/publications/optut/optut_9300.pdf">4</a> M. Teboulle. First-Order Algorithms for Convex Optimization. IPAM, 2010</li><li><a href="http://www2.isye.gatech.edu/~nemirovs/Lect_ModConvOpt.pdf">5</a> A. Ben-Tal, A. Nemirovski. Lectures on Modern Convex Optimization. Lecture Notes, 2013</li><li><a href="http://en.wikipedia.org/wiki/Subgradient#The_subgradient">6</a> http://en.wikipedia.org/wiki/Subgradient#The_subgradient</li><li><a href="http://www.princeton.edu/~sbubeck/Bubeck14.pdf">7</a> S. Bubeck. Theory of Convex Optimization for Machine Learning. Lecture Notes, 2014</li><li>[8] T. Hastie, R. Tibshirani, J. Friedman. Elements of Statistical Learning. Second Edition.</li><li><a href="http://www.ecore.be/DPs/dp_1329823186.pdf">9</a> Y. Nesterov. Subgradient Methods for Huge-Scale Optimization Problems. ECORE Discussion Paper, 2012</li><li><a href="http://www2.isye.gatech.edu/~nemirovs/MLOptChapterI.pdf">10</a> A. Juditsky, A. Nemirovski. First-Order Methods for Non-Smooth Convex Large-Scale Optimization, 1. Optimization for Machine Learning,</li><li>[11] J. F. Bonnan, J. C. Gilbert, C. Lemar√©chal, C. A. Sagastiz√°bal. Numerical Optimization: Theoretical and Practical Aspects. Second Edition, Springer, 2006</li><li><a href="https://iew3.technion.ac.il/Home/Users/becka/smoothing.pdf">12</a> A. Beck, M. Teboulle. Smoothing and First Order Methods: A Unified Framework. SIAM Journal Of Optimization, 2012</li><li><a href="http://web.stanford.edu/~boyd/papers/prox_algs.html">13</a> N. Parrikh, S. Boyd. Proximal Algorithms. Foundations and Trends in Optimization, 2014</li><li><a href="http://www.ljll.math.upmc.fr/~plc/mms1.pdf">14</a> P. L. Combette, V. R. Wajs. Signal Recovery by Proximal Forward-Backward Splitting. Multiscale Modeling and Simulation, 2005</li></ul><h2 id="credits">Credits</h2><p>The image comes from <a href="https://regularize.wordpress.com/2012/05/21/problems-solved-rip-and-nsp-are-np-hard-homotopy-for-l1-has-exponential-complexity/">regularize.wordpress</a>, and shows a 3d Mairal-Yu simplex, the equivalent of a Klee-Minty cube for the $L_1$ homotopy method.</p></div><ul class="post__social"><li><a href="https://www.facebook.com/sharer/sharer.php?u=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-facebook"></i></a></li><li><a href="https://twitter.com/intent/tweet?&text=First-order methods for regularization+http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/+by+Umayr Hassan" target="_blank"><i class="fa fa-twitter"></i></a></li><li><a href="https://plus.google.com/share?url=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-google-plus"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&source=First-order methods for regularization&summary=&url=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://www.stumbleupon.com/badge/?url=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-stumbleupon"></i></a></li><li><a href="https://www.reddit.com/submit?url=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-reddit-alien"></i></a></li><li><a href="https://www.tumblr.com/share/link?url=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-tumblr"></i></a></li><li><a href="https://www.pinterest.com/pin/create/link/?description=&media=http://umayrh.github.io/assets/images/first-regular.png&url=http://umayrh.github.io/2014/07/17/first-order-methods-for-regularization/" target="_blank"><i class="fa fa-pinterest"></i></a></li></ul></div></div><div class="section-padding--none"><div class="grid"><hr class="sep"/></div></div><div class="section-padding"><div class="grid-small"> <span class="post__author">Posted by <a href="http://umayrh.github.io" title="More By Umayr Hassan">Umayr Hassan</a></span><p class="post__bio"></p></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="http://umayrh.github.io/2016/12/23/a-fearful-sphere-whose-center-is-everywhere/"><figure class="related__img"> <img src="/assets/images/screen-shot-2016-12-16-at-10-28-14-pm.png" alt="A fearful sphere, whose center is everywhere"/></figure><div><h2 class="related__text">A fearful sphere, whose center is everywhere</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://umayrh.github.io/2014/10/06/obituaries-harold-kuhn-1925-2014/"><div><h2 class="related__text">Obituaries: Harold Kuhn (1925‚Äì2014)</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://umayrh.github.io/2014/07/24/accelerated-first-order-methods-for-regularization/"><figure class="related__img"> <img src="/assets/images/agd.png" alt="Accelerated first-order methods for regularization"/></figure><div><h2 class="related__text">Accelerated first-order methods for regularization</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get a weekly email of posts I‚Äôve added to the site.</p><form method="POST" action="&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__tags"><li><a class="footer__link" href="/tag/mathematics">Mathematics</a></li><li><a class="footer__link" href="/tag/algorithms">Algorithms</a></li><li><a class="footer__link" href="/tag/convex-optimization">Convex Optimization</a></li><li><a class="footer__link" href="/tag/java">Java</a></li><li><a class="footer__link" href="/tag/obit">Obit</a></li></ul><ul class="footer__social"><li><a href="https://www.linkedin.com/in/umayr/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://github.com/umayrh" target="_blank"><i class="fa fa-github"></i></a></li><li><a href="https://gitlab.com/umayrh" target="_blank"><i class="fa fa-gitlab"></i></a></li><li><a href="https://umayrh.wordpress.com" target="_blank"><i class="fa fa-wordpress"></i></a></li></ul></div></div></footer><section class="contact popup"><div class="popup__close"><div class="popup__exit"></div></div><div class="contact__container popup__container"><div class="contact__img"><figure class="absolute-bg" style="background-image: url(/assets/images/form_contact.jpg);"></figure></div><div class="contact__content"><div class="contact__mast section-padding--half"><div class="grid"><h2>Contact</h2></div></div><div class="section-padding--none"><hr class="sep"/></div><div class="contact__form section-padding--half"><div class="grid-xlarge"> <form id="form" class="form" action="https://formcarry.com/s/UuffFC2ATgC" method="POST"><div class="form__subcontainer"><div> <label for="form-first-name">First Name</label> <input type="text" name="first-name" id="form-first-name" required></div><div> <label for="form-last-name">Last Name</label> <input type="text" name="last-name" id="form-last-name" required></div></div><div> <label for="form-email">E-Mail</label> <input type="email" name="email" id="form-email" required></div><div> <label for="form-message">Message</label> <textarea name="message" id="form-message" rows="3"></textarea></div><div class="form__submit"><div class="form__btn"> <input type="submit" value="Send"></div></div><p class="form__message"></p></form></div></div></div></div></section><script src="/assets/js/app.min.js"></script></body></html>
  