<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Accelerated first-order methods for regularization | Sketchy Polytopes</title><meta name="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="name" content="Umayr Hassan"><meta itemprop="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="image" content="http://localhost:4000/assets/images/agd.png"><meta property="og:url" content="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><meta property="og:type" content="website"><meta property="og:title" content="Accelerated first-order methods for regularization | Sketchy Polytopes"><meta property="og:site_name" content="Sketchy Polytopes"><meta property="og:description" content="Algorithms, optimization; systems, data, and people "><meta property="og:image" content="http://localhost:4000/assets/images/agd.png"><meta name="twitter:url" content="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Accelerated first-order methods for regularization | Sketchy Polytopes"><meta name="twitter:site" content="Sketchy Polytopes"><meta name="twitter:description" content="Algorithms, optimization; systems, data, and people "><meta property="twitter:image" content="http://localhost:4000/assets/images/agd.png"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="Sketchy Polytopes" href="/feed.xml"><link rel="canonical" href="/2014/07/24/accelerated-first-order-methods-for-regularization/"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body id="accelerated-first-order-methods-for-regularization" class="post-layout"><header class="header"> <a class="header__title" href="http://localhost:4000/">Sketchy Polytopes</a><nav><ul class="header__list"><li><a href="/">Stories</a></li><li><a href="/style-guide">Style Guide</a></li><li><span class="popup__open">Contact</span></li></ul></nav></header><main class="💈"><div class="post"><article itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding--double"><div class="grid-small"><h2 itemprop="name headline">Accelerated first-order methods for regularization</h2><time class="post__date" datetime="2014-07-24T00:00:00-07:00" itemprop="datePublished">24 Jul 2014</time></div></div><div class="post__img"><div><figure class="absolute-bg" style="background-image: url('/assets/images/agd.png');"></figure></div></div><div class="post__content section-padding"><div class="grid"><div id="markdown" itemprop="articleBody"><h2 id="fista">FISTA</h2><p>Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) [Beck09] was one of the first algorithms to use Nesterov’s accelerated gradient descent to speed up the convergence of iterative shrinkage-thresholding (ISTA) from $O(\frac{\beta}{\epsilon})$ to $O(\sqrt{\frac{\beta}{\epsilon}})$.  The algorithm and proof sketch for in the previous post based were based on [Bubeck14] and [Beck09] so we’ll skip them. Instead give an R implementation and results that compares ISTA and FISTA performance (in terms of number of iterations) for various step sizes (iterations increase and step size decreases).</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ISTA</span><span class="w">
</span><span class="n">ista</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">## parameters</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
    </span><span class="c1">## helpers</span><span class="w">
    </span><span class="n">Axx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w">
    </span><span class="n">Axn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">
    </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
    </span><span class="n">iters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
 
    </span><span class="c1">## main loop</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">gradient_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w">
        </span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x</span><span class="w">
        </span><span class="n">v</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">lambda</span><span class="w">
        </span><span class="c1"># L1 prox/shrinkage-thresholding</span><span class="w">
        </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="o">-</span><span class="w"> </span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
        </span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Axx</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Axn</span><span class="w">
        </span><span class="n">iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
    </span><span class="p">}</span><span class="w">
 
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
 
</span><span class="c1">## FISTA</span><span class="w">
</span><span class="n">fista</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">## parameters</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
    </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="c1">## helpers</span><span class="w">
    </span><span class="n">Ax</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w">
    </span><span class="n">Ax_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">Ax</span><span class="p">)</span><span class="w">
    </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
    </span><span class="n">iters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
 
    </span><span class="c1">## main loop</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mu_prev</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
        </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mu_prev</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="n">gradient_x_n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w">
        </span><span class="n">z_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x_n</span><span class="w">
        </span><span class="n">v</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">lambda</span><span class="w">
        </span><span class="c1"># L1 prox/shrinkage-thresholding</span><span class="w">
        </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="n">z_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="o">-</span><span class="w"> </span><span class="n">z_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
        </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">z_prev</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">z_next</span><span class="p">)</span><span class="w">
        </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w">
        </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Ax</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Ax_norm</span><span class="w">
        </span><span class="n">iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
    </span><span class="p">}</span><span class="w">
 
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x_next</span><span class="p">,</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
 
</span><span class="n">max_iter</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">500</span><span class="p">;</span><span class="w">
</span><span class="n">beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"F"</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w">
 
</span><span class="n">res1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ista</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="p">)</span><span class="w">
</span><span class="n">res2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fista</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="p">)</span><span class="w">
 
</span><span class="n">plot</span><span class="p">(</span><span class="n">res1</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="n">res1</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"iterations"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"error"</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">res2</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="n">res2</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><table><thead><tr><th style="text-align: center"><img src="http://umayrh.files.wordpress.com/2014/07/ista500.png?w=300" alt="ISTA vs FISTA, 500 iterations" /></th><th style="text-align: center"><img src="http://umayrh.files.wordpress.com/2014/07/ista5000.png?w=300" alt="ISTA vs FISTA, 5000 iterations" /></th></tr></thead><tbody><tr><td style="text-align: center">ISTA vs FISTA, 500 iterations</td><td style="text-align: center">5000 iterations</td></tr></tbody></table><p>Note the non-monotonic convergence in FISTA’s case - [Beck09b, Teboulle10] describe a simple change to the algorithm to fix that. Another interesting problem with FISTA is the dependence on the worst-case smoothness parameter β in the algorithm, which can substantially reduce convergence rate for large β. This is addressed in [Katya14] using a backtracking strategy that  to improve the dependence from worst-case to average “local composite Lipschitz constant for ∇f”, which can have a much smaller value, implying a larger step size. Another solution is presented in [Baes12].</p><p>Recall that ISTA, and hence FISTA, solve the following optimization problems:</p><script type="math/tex; mode=display">\min{( \frac{1}{2} \|Ax - b\|^2 +\lambda \|x\| )}</script><p>FISTA works for non-smooth objectives as long as they can be formulated as the sum of a smooth and a non-smooth function [Section 2, Beck09; Section 3, Tseng08]. Are there efficient first-order algorithms for a larger class of non-smooth problems?</p><h2 id="nesta">NESTA</h2><p>NESTA Becker09 solves the following optimization problem that FISTA cannot since the objective function is non-composite non-smooth:</p><script type="math/tex; mode=display">\min{\|x\|_1} \mbox{s.t.} \|Ax - b\|_2 \le \epsilon</script><p>This algorithm draws on [Nesterov05], which introduces a way to create smooth approximations of certain non-smooth function. This approximation, coupled with an accelerated gradient descent method, gives good convergence for optimizing a large class of non-smooth functions (though not as good as FISTA or Nesterov07). We’ll review  [Nesterov05] via [Becker09] before returning to NESTA.</p><h2 id="smoothing-non-smooth-functions">Smoothing non-smooth functions</h2><p>Nesterov considers a class of functions that can be represented as [Section 2, [Nesterov05]; Section 2, Becker09; Nesterov08]</p><script type="math/tex; mode=display">\max_{u \in Q_d}\{(Ax - b)^{T}u - \phi{(u)}\}</script><p>where $\phi{(u)}$ is a convex function and $Q_d$ is the dual domain (convex, closed) of a function $f(x)$ minimized over a domain $Q_p$. The representation is similar to the <a href="http://en.wikipedia.org/wiki/Convex_conjugate">convex conjugate</a>, $f^{\ast}(u)$,  of this function. Indeed Nesterov considers using $\phi{(u)} = f^{\ast}(u)$, but then argues that such $\phi{(u)}$ might not be a simple enough function. This might be true in general, but as we’ll see that in the special case of $L_1$ regularization, $\phi{(u)} = f^{\ast}(u)$ will work.</p><p>Nesterov then presents a smooth approximation of the function</p><script type="math/tex; mode=display">f_{\mu}(x) = \max_{u \in Q_d}\{(Ax - b)^{T}u - \phi{(u)} - \mu\ prox_{\alpha}(u)\}</script><p>where μ is the smoothness parameter, $prox_{\alpha}(u)$ is an $\alpha$-strongly convex proximal function over the dual domain $Q_d$. In the original paper [Nesterov05], Nesterov actually allows for a composite function approximation, that is $f(x) = fº(x) + f \mu{(x)}$ where $fº(x)$ is some smooth convex function. We’ll follow Becker09 in assuming that $fº(x) = 0$. Beck12 extends this smoothing framework to optimize an even broader class of non-smooth functions.</p><p>The important upshot is that (1) this function is smooth with factor $\frac{|A|}{\mu \alpha}$  [Theorem 1, [Nesterov05]], where $|A|$ is the operator norm of $A$, and (2) that $f_{\mu}(x)$ is a uniform smooth approximation of $f(x)$ for $\mu &gt; 0$. So now we can use an optimal gradient descent for smooth optimization to minimize $f_{\mu}(x)$.</p><h2 id="smooth-optimization">Smooth optimization</h2><p>Section 3 in [Nesterov05] describes an optimal first-order algorithm to minimize a $\beta$-smooth convex function $f(x)$ using an $\alpha$-strongly convex proximal function $prox_{\alpha}(x)$.</p><script type="math/tex; mode=display">y_k = \mathrm{argmin}_{y}(\nabla{f(x_k)}^{T} + \frac{1}{2}\beta \|y - x_k\|^2)) \\ z_k=\mathrm{argmin}_{x}(\frac{\beta}{\alpha}\ prox_{\alpha}(x)+\sum_{i=0}^{k}(\frac{i+1}{2}[\nabla{f(x_i)} + \nabla{f(x_i)}^{T}(x - x_i)])) \\ x_{k+1} = \frac{2}{k+1}z_k + \frac{k+1}{k+3}y_k</script><p>This is shown Theorem 2, [Nesterov05] to converge at a rate</p><script type="math/tex; mode=display">f(y_k) - f(x^\ast) \le \frac{4 \beta\ prox_{\alpha}(x^\ast)}{\alpha (k+1)(k+2)}</script><p>This algorithm is more complicated than the earlier ones Nesterov83, Nesterov88 - especially when compared to the version FISTA uses (see <a href="http://umayrh.wordpress.com/2014/07/21/accelerating-first-order-methods/#more-756">previous post</a>). The only advantage seems to be a somewhat better convergence rate by a factor of α at the cost of solving two minimization problems (instead of one as in FISTA).</p><p>Applying this algorithm to optimize $f_{\mu}(x)$ Theorem 3, [Nesterov05] gives $O(1/k)$ convergence for the optimal value of $\mu$ since:</p><script type="math/tex; mode=display">f(y_k) - f(x^\ast) \le O(1)\mu + \frac{O(1)}{\mu (k+1)^2} + \frac{O(1)}{(k+1)^2}</script><h2 id="nesterovs-algorithm">Nesterov’s Algorithm</h2><p>[Becker09] call their application of [Nesterov05] to $L_1$ regularization, NESTA. They show that:</p><p>(1) Optimizing over the convex conjugate of $L_1$ norm when combined with an appropriate proximal function yields a simple, well-known function called the <a href="http://en.wikipedia.org/wiki/Huber_loss_function">Huber loss</a> function. It is analytic and can be computed very efficiently, which makes it trivial to implement gradient descent (quite like the shrinkage-thresholding operation in ISTA/FISTA).</p><script type="math/tex; mode=display">\|x\| = \max_{u \in Q_d} \{(u^{T}x)\} \mbox{ where } Q_d = \{u : \|u\|_{\infty} \le 1\}</script><p>Choosing $prox_{\alpha}(u)$ to be the squared Euclidean distance with $\alpha = 1$ (cf. Section 4.2, [Nesterov05]),</p><p>$f_{\mu}(x) = \max_{u \in Q_d}{(u^{T}x) - \frac{\mu}{2}|u|_2^2} = \begin{cases}\frac{x^2}{2\mu}, &amp; |x| &lt; \mu \ |x| - \frac{\mu}{2}, &amp; otherwise\end{cases} $</p><p>$\nabla{f(x)[i]} = \begin{cases}\frac{x[i]}{\mu}, &amp; |x[i]| &lt; \mu \ sgn(x[i]), &amp; otherwise\end{cases}$</p><p>(2) $y_k$ and $z_k$ updates in the original algorithm need to be modified taking into account the $L_2$ constraint. This is done using the Lagrangian form of each minimization that, under certain assumptions, allows expressing Lagrangian variables in terms of $\epsilon$.</p><p>(3) Another interesting idea is the use of “continuation.” Starting with a large value of the smoothing parameter and multiplicatively decreasing after each iteration helps converge more quickly (although the theoretical convergence rate stays the same).</p><p>Here’s R code for the unconstrained version of NESTA based on FISTA’s version of accelerated gradient descent:</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nesta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">## parameters</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
    </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="n">smooth</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.9</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">quot</span><span class="p">;</span><span class="n">I</span><span class="o">&amp;</span><span class="n">quot</span><span class="p">;)</span><span class="w">
    </span><span class="c1">## helpers</span><span class="w">
    </span><span class="n">Ax</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w">
    </span><span class="n">Ax_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">Ax</span><span class="p">)</span><span class="w">
    </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
    </span><span class="n">iters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
 
    </span><span class="c1">## main loop</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mu_prev</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
        </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mu_prev</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="c1"># grad_huber1 + grad_huber2 = gradient of Huber function</span><span class="w">
        </span><span class="n">grad_huber1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="w"> </span><span class="n">smooth</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">smooth</span><span class="w">
        </span><span class="n">grad_huber2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="o">=</span><span class="w"> </span><span class="n">smooth</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ifelse</span><span class="p">(</span><span class="n">x_next</span><span class="o">==</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">x_next</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="n">x_next</span><span class="p">)))</span><span class="w">
        </span><span class="n">gradient_x_n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_huber1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_huber2</span><span class="w">
        </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x_n</span><span class="w">
        </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">z_prev</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">z_next</span><span class="p">)</span><span class="w">
        </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w">
        </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="n">smooth</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">smooth</span><span class="w">
        </span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Ax</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Ax_norm</span><span class="w">
        </span><span class="n">iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
    </span><span class="p">}</span><span class="w">
 
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x_next</span><span class="p">,</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h2 id="postscript-02092014">Postscript 02/09/2014</h2><p>Building on the message-passing (aka belief propagation) algorithms introduced in [Donoho09], [Mousavi13] improve the convergence rate of iterative-shrinkage thresholding algorithm (ISTA) to $O(e^{-t})$. This was made possible by finding the optimal values of the regularization parameter, λ, at each iteration under the assumption of Gaussian error distribution (which is probably why it escapes the upper-bounds for first-order methods).</p><h2 id="references">References</h2><ul><li><a href="http://arxiv.org/pdf/1207.3951v1.pdf">Baes12</a> M. Baes, M. Burgisser. An acceleration procedure for optimal first-order methods. 2012</li><li><a href="http://mechroom.technion.ac.il/~becka/papers/71654.pdf">Beck09</a> A. Beck, M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal of Imaging Sciences, 2009</li><li><a href="http://www.math.tau.ac.il/~teboulle/papers/tlv.pdf">Beck09b</a> A. Beck, M. Teboulle. Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems. IEEE Transactions on Image Processing, 2009.</li><li><a href="https://iew3.technion.ac.il/Home/Users/becka/smoothing.pdf">Beck12</a> A. Beck, M. Teboulle. Smoothing and First Order Methods: A Unified Approach. SIAM Journal of Optimization, 2012.</li><li><a href="http://statweb.stanford.edu/~candes/nesta/NESTA.pdf">Becker09</a> S. Becker, J. Bobin, E. Candes. NESTA: A Fast and Accurate First-Order Method for Sparse Recovery. Technical Report, Caltech, 2009</li><li><a href="http://www.princeton.edu/~sbubeck/Bubeck14.pdf">Bubeck14</a> S. Bubeck, Theory of Convex Optimization for Machine Learning</li><li><a href="http://www.ece.rice.edu/~mam15/amp_pnas.pdf">Donoho09</a> D. Donoho, A. Maleki, A. Montanari. Message-passing algorithms for compressed sensing. Proceedings of National Academy of Sciences, 2009.</li><li><a href="http://arxiv.org/pdf/1311.0035v1.pdf">Mousavi13</a> A. Mousavi, A. Maleki, R. Baranuick. Parameterless optimal approximate message passing. CoRR, 2013</li><li>[Nesterov83] Y. Nesterov. A method for solving a convex programming problem with convergence rate O(1/k2). Dokaldy AN SSR, 1983</li><li>[Nesterov88] Y. Nesterov. On an approach to the construction of optimal methods of minimization of smooth convex functions. Ekonom. i. Mat. Metody, 1988</li><li>[Nesterov04] Y. Nesterov. Introductory Lectures On Convex Programming: A Basic Course. Kluwer Academic Publishers, 2004</li><li><a href="http://luthuli.cs.uiuc.edu/~daf/courses/Optimization/MRFpapers/[Nesterov05].pdf">Nesterov05</a> Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 2005.</li><li><a href="http://www.optimization-online.org/DB_FILE/2007/09/1784.pdf">Nesterov07</a> Y. Nesterov. Gradient methods for minimizing composite objective function. Report, CORE, 2007</li><li><a href="http://galton.uchicago.edu/~lekheng/courses/31060s13/nesterov.pdf">Nesterov08</a> Y. Nesterov. How to advance in Structural Convex Optimization. Optima, 2008</li><li><a href="http://www.optimization-online.org/DB_FILE/2011/04/3004.pdf">Katya14</a> K. Scheinberg, D. Goldfarb, X. Bai. Fast First-Order Methods for Composite Convex Optimization with Backtracking. Foundations of Computational Mathematics, 2014.</li><li><a href="https://www.ipam.ucla.edu/publications/optut/optut_9300.pdf">Teboulle10</a>. M. Teboulle. First-Order Methods for Optimization. IPAM Optimization Tutorials, 2010</li><li><a href="http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf">Tseng08</a> P. Tseng. On Accelerated Proximal Gradient Algorithms for Convex-Concave Optimization. SIAM Journal of Optimization, 2008</li></ul><h2 id="implementations">Implementations</h2><ul><li><a href="http://web.stanford.edu/~boyd/papers/prox_algs/lasso.html">Stanford, LASSO</a></li><li><a href="http://www.caam.rice.edu/~optimization/disparse/LASSO/FISTA/pFistaLasso.html">Rice, LASSO-FISTA</a></li><li><a href="https://github.com/slipguru/l1l2py">Genoa, L1-L2 in Python</a></li><li><a href="http://www.eecs.berkeley.edu/~yang/software/l1benchmark/">UCB, L1 Benchmark</a></li><li><a href="http://www.ece.rice.edu/~tag7/Tom_Goldstein/CGIST.html">Rice, CGIST</a></li><li><a href="http://www.mit.edu/~dimitrib/PTseng/papers/apg_alg.m">MIT, Approx. Proximal Gradient</a></li><li><a href="http://www.math.ucla.edu/~wotaoyin/software.html">UCLA</a></li><li><a href="http://www.mathworks.com/matlabcentral/fileexchange/16204-toolbox-sparse-optmization/content/toolbox_optim/perform_fb.m">MATLAB, Optim Toolbox</a></li><li><a href="https://github.com/gpeyre/numerical-tours/tree/master/matlab/solutions">Numerical algorithms in MATLAB</a></li><li><a href="http://www.numerical-tours.com">Numerical Tours</a></li></ul><h2 id="credits">Credits</h2><p>Main image from <a href="https://lossfunctions.tumblr.com/image/126914340062">lossfunctions.tumblr</a>.</p></div><ul class="post__social"><li><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-facebook"></i></a></li><li><a href="https://twitter.com/intent/tweet?&text=Accelerated first-order methods for regularization+http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/+by+Umayr Hassan" target="_blank"><i class="fa fa-twitter"></i></a></li><li><a href="https://plus.google.com/share?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-google-plus"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&source=Accelerated first-order methods for regularization&summary=&url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://www.stumbleupon.com/badge/?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-stumbleupon"></i></a></li><li><a href="https://www.reddit.com/submit?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-reddit-alien"></i></a></li><li><a href="https://www.tumblr.com/share/link?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-tumblr"></i></a></li><li><a href="https://www.pinterest.com/pin/create/link/?description=&media=http://localhost:4000/assets/images/agd.png&url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-pinterest"></i></a></li></ul></div></div><div class="section-padding--none"><div class="grid"><hr class="sep"/></div></div><div class="section-padding"><div class="grid-small"> <span class="post__author">Posted by <a href="http://umayrh.github.io" title="More By Umayr Hassan">Umayr Hassan</a></span><p class="post__bio"></p></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="http://localhost:4000/2016/12/23/a-fearful-sphere-whose-center-is-everywhere/"><figure class="related__img"> <img src="/assets/images/screen-shot-2016-12-16-at-10-28-14-pm.png" alt="A fearful sphere, whose center is everywhere"/></figure><div><h2 class="related__text">A fearful sphere, whose center is everywhere</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/10/06/obituaries-harold-kuhn-1925-2014/"><div><h2 class="related__text">Obituaries: Harold Kuhn (1925–2014)</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><figure class="related__img"> <img src="/assets/images/agd.png" alt="Accelerated first-order methods for regularization"/></figure><div><h2 class="related__text">Accelerated first-order methods for regularization</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get a weekly email of posts I’ve added to the site.</p><form method="POST" action="&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__tags"><li><a class="footer__link" href="/tag/mathematics">Mathematics</a></li><li><a class="footer__link" href="/tag/algorithms">Algorithms</a></li><li><a class="footer__link" href="/tag/convex-optimization">Convex Optimization</a></li><li><a class="footer__link" href="/tag/java">Java</a></li><li><a class="footer__link" href="/tag/obit">Obit</a></li></ul><ul class="footer__social"><li><a href="https://www.linkedin.com/in/umayr/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://github.com/umayrh" target="_blank"><i class="fa fa-github"></i></a></li><li><a href="https://gitlab.com/umayrh" target="_blank"><i class="fa fa-gitlab"></i></a></li></ul></div></div></footer><section class="contact popup"><div class="popup__close"><div class="popup__exit"></div></div><div class="contact__container popup__container"><div class="contact__img"><figure class="absolute-bg" style="background-image: url(/assets/images/form_contact.jpg);"></figure></div><div class="contact__content"><div class="contact__mast section-padding--half"><div class="grid"><h2>Contact</h2></div></div><div class="section-padding--none"><hr class="sep"/></div><div class="contact__form section-padding--half"><div class="grid-xlarge"> <form id="form" class="form" action="https://formcarry.com/s/UuffFC2ATgC" method="POST"><div class="form__subcontainer"><div> <label for="form-first-name">First Name</label> <input type="text" name="first-name" id="form-first-name" required></div><div> <label for="form-last-name">Last Name</label> <input type="text" name="last-name" id="form-last-name" required></div></div><div> <label for="form-email">E-Mail</label> <input type="email" name="email" id="form-email" required></div><div> <label for="form-message">Message</label> <textarea name="message" id="form-message" rows="3"></textarea></div><div class="form__submit"><div class="form__btn"> <input type="submit" value="Send"></div></div><p class="form__message"></p></form></div></div></div></div></section><script src="/assets/js/app.min.js"></script></body></html>
  