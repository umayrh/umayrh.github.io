<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Accelerated first-order methods for regularization | Sketchy Polytopes</title><meta name="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="name" content="Umayr Hassan"><meta itemprop="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="image" content="http://localhost:4000/assets/images/agd.png"><meta property="og:url" content="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><meta property="og:type" content="website"><meta property="og:title" content="Accelerated first-order methods for regularization | Sketchy Polytopes"><meta property="og:site_name" content="Sketchy Polytopes"><meta property="og:description" content="Algorithms, optimization; systems, data, and people "><meta property="og:image" content="http://localhost:4000/assets/images/agd.png"><meta name="twitter:url" content="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Accelerated first-order methods for regularization | Sketchy Polytopes"><meta name="twitter:site" content="Sketchy Polytopes"><meta name="twitter:description" content="Algorithms, optimization; systems, data, and people "><meta property="twitter:image" content="http://localhost:4000/assets/images/agd.png"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="Sketchy Polytopes" href="/feed.xml"><link rel="canonical" href="/2014/07/24/accelerated-first-order-methods-for-regularization/"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body id="accelerated-first-order-methods-for-regularization" class="post-layout"><header class="header"> <a class="header__title" href="http://localhost:4000/">Sketchy Polytopes</a><nav><ul class="header__list"><li><a href="/">Stories</a></li><li><a href="/style-guide">Style Guide</a></li><li><span class="popup__open">Contact</span></li></ul></nav></header><main class="üíà"><div class="post"><article itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding--double"><div class="grid-small"><h2 itemprop="name headline">Accelerated first-order methods for regularization</h2><time class="post__date" datetime="2014-07-24T00:00:00-07:00" itemprop="datePublished">24 Jul 2014</time></div></div><div class="post__img"><div><figure class="absolute-bg" style="background-image: url('/assets/images/agd.png');"></figure></div></div><div class="post__content section-padding"><div class="grid"><div id="markdown" itemprop="articleBody"><h2 id="fista">FISTA</h2><p>Fast Iterative Shrinkage-Thresholding Algorithm (FISTA) [Beck09]¬†was one of the first algorithms to use Nesterov‚Äôs accelerated gradient descent to speed up the convergence of iterative shrinkage-thresholding (ISTA) from $O(\frac{\beta}{\epsilon})$ to $O(\sqrt{\frac{\beta}{\epsilon}})$. ¬†The algorithm and proof sketch for in the previous post based were based on [Bubeck14] and [Beck09] so we‚Äôll skip them. Instead give an R implementation and results that compares ISTA and FISTA performance (in terms of number of iterations) for various step sizes (iterations increase and step size decreases).</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1">## ISTA</span><span class="w">
</span><span class="n">ista</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">## parameters</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
    </span><span class="c1">## helpers</span><span class="w">
    </span><span class="n">Axx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w">
    </span><span class="n">Axn</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="p">)</span><span class="w">
    </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
    </span><span class="n">iters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
 
    </span><span class="c1">## main loop</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">gradient_x</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w">
        </span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x</span><span class="w">
        </span><span class="n">v</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">lambda</span><span class="w">
        </span><span class="c1"># L1 prox/shrinkage-thresholding</span><span class="w">
        </span><span class="n">xx</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="o">-</span><span class="w"> </span><span class="n">xx_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
        </span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">xx</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Axx</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Axn</span><span class="w">
        </span><span class="n">iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
    </span><span class="p">}</span><span class="w">
 
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">xx</span><span class="p">,</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
 
</span><span class="c1">## FISTA</span><span class="w">
</span><span class="n">fista</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">## parameters</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
    </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="c1">## helpers</span><span class="w">
    </span><span class="n">Ax</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w">
    </span><span class="n">Ax_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">Ax</span><span class="p">)</span><span class="w">
    </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
    </span><span class="n">iters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
 
    </span><span class="c1">## main loop</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mu_prev</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
        </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mu_prev</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="n">gradient_x_n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w">
        </span><span class="n">z_tmp</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x_n</span><span class="w">
        </span><span class="n">v</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">lambda</span><span class="w">
        </span><span class="c1"># L1 prox/shrinkage-thresholding</span><span class="w">
        </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="n">z_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">pmax</span><span class="p">(</span><span class="o">-</span><span class="w"> </span><span class="n">z_tmp</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">v</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
        </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">z_prev</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">z_next</span><span class="p">)</span><span class="w">
        </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w">
        </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Ax</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Ax_norm</span><span class="w">
        </span><span class="n">iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
    </span><span class="p">}</span><span class="w">
 
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x_next</span><span class="p">,</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
 
</span><span class="n">max_iter</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">500</span><span class="p">;</span><span class="w">
</span><span class="n">beta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="o">=</span><span class="s2">"F"</span><span class="p">)</span><span class="o">^</span><span class="m">2</span><span class="w">
 
</span><span class="n">res1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">ista</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="p">)</span><span class="w">
</span><span class="n">res2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">fista</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">max_iter</span><span class="p">)</span><span class="w">
 
</span><span class="n">plot</span><span class="p">(</span><span class="n">res1</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="n">res1</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"red"</span><span class="p">,</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"iterations"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"error"</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span><span class="n">lines</span><span class="p">(</span><span class="n">res2</span><span class="p">[[</span><span class="m">3</span><span class="p">]],</span><span class="w"> </span><span class="n">res2</span><span class="p">[[</span><span class="m">2</span><span class="p">]],</span><span class="w"> </span><span class="n">col</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"blue"</span><span class="p">,</span><span class="w"> </span><span class="n">type</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s1">'l'</span><span class="p">,</span><span class="w"> </span><span class="n">lty</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">1</span><span class="p">)</span><span class="w">
</span></code></pre></div></div><table><thead><tr><th style="text-align: center"><img src="http://umayrh.files.wordpress.com/2014/07/ista500.png?w=300" alt="ISTA vs FISTA, 500 iterations" /></th><th style="text-align: center"><img src="http://umayrh.files.wordpress.com/2014/07/ista5000.png?w=300" alt="ISTA vs FISTA, 5000 iterations" /></th></tr></thead><tbody><tr><td style="text-align: center">ISTA vs FISTA, 500 iterations</td><td style="text-align: center">5000 iterations</td></tr></tbody></table><p>Note the non-monotonic convergence in FISTA‚Äôs case - [Beck09b, Teboulle10] describe¬†a simple change to the algorithm to fix that. Another interesting problem with FISTA is the dependence on the worst-case smoothness parameter Œ≤¬†in the algorithm, which can substantially reduce convergence rate for large¬†Œ≤. This is addressed in [Katya14] using a backtracking strategy that ¬†to improve¬†the dependence from worst-case to average ‚Äúlocal composite Lipschitz constant for ‚àáf‚Äù, which can have a much smaller value, implying a larger step size. Another solution is presented in [Baes12].</p><p>Recall that ISTA, and hence FISTA, solve the following optimization problems:</p><script type="math/tex; mode=display">\min{( \frac{1}{2} \|Ax - b\|^2 +\lambda \|x\| )}</script><p>FISTA works for non-smooth objectives as long as they can be formulated as the sum of a smooth and a non-smooth function [Section 2, Beck09; Section 3, Tseng08]. Are there¬†efficient first-order algorithms for a larger class of non-smooth problems?</p><h2 id="nesta">NESTA</h2><p>NESTA Becker09¬†solves the following optimization problem that FISTA cannot since the objective function is non-composite non-smooth:</p><script type="math/tex; mode=display">\min{\|x\|_1} \mbox{s.t.} \|Ax - b\|_2 \le \epsilon</script><p>This algorithm draws on [Nesterov05], which introduces a way to create smooth approximations of certain non-smooth function. This approximation, coupled with an accelerated gradient descent method, gives good¬†convergence for optimizing a large class of non-smooth functions (though not as good as FISTA or Nesterov07). We‚Äôll review¬† [Nesterov05] via [Becker09] before returning to NESTA.</p><h2 id="smoothing-non-smooth-functions">Smoothing non-smooth functions</h2><p>Nesterov considers¬†a class of functions that can be represented as¬†[Section 2, [Nesterov05]; Section 2, Becker09; Nesterov08]</p><script type="math/tex; mode=display">\max_{u \in Q_d}\{(Ax - b)^{T}u - \phi{(u)}\}</script><p>where $\phi{(u)}$ is a convex function and $Q_d$¬†is the dual domain (convex, closed) of a function $f(x)$ minimized over a domain $Q_p$. The representation is similar to the <a href="http://en.wikipedia.org/wiki/Convex_conjugate">convex conjugate</a>, $f^{\ast}(u)$,¬† of this function. Indeed Nesterov considers using¬†$\phi{(u)} = f^{\ast}(u)$, but then argues that¬†such¬†$\phi{(u)}$ might not be a simple enough function. This might be true in general, but as we‚Äôll see that¬†in¬†the special case of $L_1$ regularization,¬†$\phi{(u)} = f^{\ast}(u)$ will work.</p><p>Nesterov¬†then presents a smooth approximation of the function</p><script type="math/tex; mode=display">f_{\mu}(x) = \max_{u \in Q_d}\{(Ax - b)^{T}u - \phi{(u)} - \mu\ prox_{\alpha}(u)\}</script><p>where¬†Œº is the smoothness parameter, $prox_{\alpha}(u)$ is an $\alpha$-strongly convex proximal function over the dual domain $Q_d$. In the original paper [Nesterov05], Nesterov actually allows for a composite function approximation,¬†that is $f(x) = f¬∫(x) + f \mu{(x)}$ where¬†$f¬∫(x)$ is some smooth convex function. We‚Äôll follow Becker09 in assuming that¬†$f¬∫(x) = 0$. Beck12 extends this smoothing framework to optimize an even broader class of non-smooth functions.</p><p>The important upshot is that (1) this function is smooth with factor $\frac{|A|}{\mu \alpha}$¬† [Theorem 1, [Nesterov05]],¬†where $|A|$ is the operator norm of $A$, and (2) that $f_{\mu}(x)$ is a uniform smooth approximation of $f(x)$ for $\mu &gt; 0$. So now we can use an optimal gradient descent for smooth¬†optimization to minimize $f_{\mu}(x)$.</p><h2 id="smooth-optimization">Smooth optimization</h2><p>Section 3 in [Nesterov05] describes an optimal first-order¬†algorithm to minimize a $\beta$-smooth convex function $f(x)$ using an $\alpha$-strongly convex proximal function $prox_{\alpha}(x)$.</p><script type="math/tex; mode=display">y_k = \mathrm{argmin}_{y}(\nabla{f(x_k)}^{T} + \frac{1}{2}\beta \|y - x_k\|^2)) \\ z_k=\mathrm{argmin}_{x}(\frac{\beta}{\alpha}\ prox_{\alpha}(x)+\sum_{i=0}^{k}(\frac{i+1}{2}[\nabla{f(x_i)} + \nabla{f(x_i)}^{T}(x - x_i)])) \\ x_{k+1} = \frac{2}{k+1}z_k + \frac{k+1}{k+3}y_k</script><p>This is shown Theorem 2, [Nesterov05] to converge at a rate</p><script type="math/tex; mode=display">f(y_k) - f(x^\ast) \le \frac{4 \beta\ prox_{\alpha}(x^\ast)}{\alpha (k+1)(k+2)}</script><p>This algorithm is more complicated than the earlier ones Nesterov83, Nesterov88 - especially when compared to the version FISTA uses (see <a href="http://umayrh.wordpress.com/2014/07/21/accelerating-first-order-methods/#more-756">previous post</a>). The only advantage seems to be a somewhat better convergence rate by a factor of¬†Œ± at the cost of solving two minimization problems (instead of one as in FISTA).</p><p>Applying this algorithm to optimize $f_{\mu}(x)$ Theorem 3, [Nesterov05] gives $O(1/k)$ convergence for the optimal value of $\mu$¬†since:</p><script type="math/tex; mode=display">f(y_k) - f(x^\ast) \le O(1)\mu + \frac{O(1)}{\mu (k+1)^2} + \frac{O(1)}{(k+1)^2}</script><h2 id="nesterovs-algorithm">Nesterov‚Äôs Algorithm</h2><p>[Becker09] call their application of [Nesterov05] to $L_1$ regularization, NESTA. They show that:</p><p>(1) Optimizing over the¬†convex conjugate of $L_1$ norm when combined with an appropriate proximal function yields¬†a¬†simple, well-known function called the¬†<a href="http://en.wikipedia.org/wiki/Huber_loss_function">Huber loss</a>¬†function. It is analytic and can be computed¬†very efficiently, which makes it trivial to implement gradient descent (quite like the shrinkage-thresholding operation in ISTA/FISTA).</p><script type="math/tex; mode=display">\|x\| = \max_{u \in Q_d} \{(u^{T}x)\} \mbox{ where } Q_d = \{u : \|u\|_{\infty} \le 1\}</script><p>Choosing¬†$prox_{\alpha}(u)$ to be¬†the squared Euclidean distance with¬†$\alpha = 1$ (cf. Section 4.2, [Nesterov05]),</p><p>$f_{\mu}(x) = \max_{u \in Q_d}{(u^{T}x) - \frac{\mu}{2}|u|_2^2} = \begin{cases}\frac{x^2}{2\mu}, &amp; |x| &lt; \mu \ |x| - \frac{\mu}{2}, &amp; otherwise\end{cases} $</p><p>$\nabla{f(x)[i]} = \begin{cases}\frac{x[i]}{\mu}, &amp; |x[i]| &lt; \mu \ sgn(x[i]), &amp; otherwise\end{cases}$</p><p>(2) $y_k$ and $z_k$ updates in the original algorithm need to be modified taking into account the $L_2$ constraint. This is done using the Lagrangian form of each minimization that, under certain assumptions, allows¬†expressing Lagrangian variables in terms of $\epsilon$.</p><p>(3) Another interesting idea is the use of ‚Äúcontinuation.‚Äù Starting with a large value of the smoothing parameter and multiplicatively decreasing after each iteration helps converge more quickly (although the theoretical convergence rate stays¬†the same).</p><p>Here‚Äôs R code for the unconstrained version of NESTA based on FISTA‚Äôs version of accelerated gradient descent:</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">nesta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="k">function</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="n">x</span><span class="p">,</span><span class="w"> </span><span class="n">beta</span><span class="p">,</span><span class="w"> </span><span class="n">iter</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="m">100</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
    </span><span class="c1">## parameters</span><span class="w">
    </span><span class="n">eta</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">beta</span><span class="w">
    </span><span class="n">lambda</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">1</span><span class="w">
    </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">)</span><span class="w">
    </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w">
    </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0</span><span class="w">
    </span><span class="n">smooth</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.9</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="p">,</span><span class="w"> </span><span class="o">&amp;</span><span class="n">quot</span><span class="p">;</span><span class="n">I</span><span class="o">&amp;</span><span class="n">quot</span><span class="p">;)</span><span class="w">
    </span><span class="c1">## helpers</span><span class="w">
    </span><span class="n">Ax</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x</span><span class="w">
    </span><span class="n">Ax_norm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">Ax</span><span class="p">)</span><span class="w">
    </span><span class="n">error</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
    </span><span class="n">iters</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">vector</span><span class="p">(</span><span class="n">length</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">iter</span><span class="p">)</span><span class="w">
 
    </span><span class="c1">## main loop</span><span class="w">
    </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">i</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="n">iter</span><span class="p">)</span><span class="w"> </span><span class="p">{</span><span class="w">
        </span><span class="n">mu_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="nf">sqrt</span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">mu_prev</span><span class="o">^</span><span class="m">2</span><span class="p">))</span><span class="w">
        </span><span class="n">gamma</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="m">1</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">mu_prev</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="c1"># grad_huber1 + grad_huber2 = gradient of Huber function</span><span class="w">
        </span><span class="n">grad_huber1</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="n">lt</span><span class="p">;</span><span class="w"> </span><span class="n">smooth</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">smooth</span><span class="w">
        </span><span class="n">grad_huber2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="p">(</span><span class="nf">abs</span><span class="p">(</span><span class="n">x_next</span><span class="p">)</span><span class="w"> </span><span class="o">&amp;</span><span class="n">gt</span><span class="p">;</span><span class="o">=</span><span class="w"> </span><span class="n">smooth</span><span class="p">)</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">ifelse</span><span class="p">(</span><span class="n">x_next</span><span class="o">==</span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="m">0</span><span class="p">,</span><span class="w"> </span><span class="p">(</span><span class="n">x_next</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="nf">abs</span><span class="p">(</span><span class="n">x_next</span><span class="p">)))</span><span class="w">
        </span><span class="n">gradient_x_n</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">A</span><span class="p">)</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="p">(</span><span class="w"> </span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">y</span><span class="w"> </span><span class="p">)</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_huber1</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">grad_huber2</span><span class="w">
        </span><span class="n">z_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">eta</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">gradient_x_n</span><span class="w">
        </span><span class="n">x_next</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w"> </span><span class="o">+</span><span class="w"> </span><span class="n">gamma</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="p">(</span><span class="n">z_prev</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">z_next</span><span class="p">)</span><span class="w">
        </span><span class="n">z_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">z_next</span><span class="w">
        </span><span class="n">mu_prev</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">mu_next</span><span class="w">
        </span><span class="n">smooth</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">0.5</span><span class="w"> </span><span class="o">*</span><span class="w"> </span><span class="n">smooth</span><span class="w">
        </span><span class="n">error</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">norm</span><span class="p">(</span><span class="n">A</span><span class="w"> </span><span class="o">%*%</span><span class="w"> </span><span class="n">x_next</span><span class="w"> </span><span class="o">-</span><span class="w"> </span><span class="n">Ax</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="w"> </span><span class="n">Ax_norm</span><span class="w">
        </span><span class="n">iters</span><span class="p">[</span><span class="n">i</span><span class="p">]</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">i</span><span class="w">
    </span><span class="p">}</span><span class="w">
 
    </span><span class="nf">return</span><span class="p">(</span><span class="nf">list</span><span class="p">(</span><span class="n">x_next</span><span class="p">,</span><span class="w"> </span><span class="n">error</span><span class="p">,</span><span class="w"> </span><span class="n">iters</span><span class="p">))</span><span class="w">
</span><span class="p">}</span><span class="w">
</span></code></pre></div></div><h2 id="postscript-02092014">Postscript 02/09/2014</h2><p>Building on the message-passing (aka belief propagation) algorithms introduced in [Donoho09], [Mousavi13] improve the convergence rate of iterative-shrinkage thresholding¬†algorithm (ISTA) to $O(e^{-t})$. This was made possible by finding the optimal values of the regularization parameter,¬†Œª, at each iteration under the assumption of Gaussian error distribution (which is probably why it escapes the upper-bounds for first-order methods).</p><h2 id="references">References</h2><ul><li><a href="http://arxiv.org/pdf/1207.3951v1.pdf">Baes12</a> M. Baes, M. Burgisser. An acceleration procedure for optimal first-order methods. 2012</li><li><a href="http://mechroom.technion.ac.il/~becka/papers/71654.pdf">Beck09</a>¬†A. Beck, M. Teboulle. A Fast Iterative Shrinkage-Thresholding Algorithm for Linear Inverse Problems. SIAM Journal of Imaging Sciences, 2009</li><li><a href="http://www.math.tau.ac.il/~teboulle/papers/tlv.pdf">Beck09b</a> A. Beck, M. Teboulle.¬†Fast Gradient-Based Algorithms for Constrained Total Variation Image Denoising and Deblurring Problems. IEEE Transactions on Image Processing, 2009.</li><li><a href="https://iew3.technion.ac.il/Home/Users/becka/smoothing.pdf">Beck12</a> A. Beck, M. Teboulle. Smoothing and First Order Methods: A Unified Approach. SIAM Journal of Optimization, 2012.</li><li><a href="http://statweb.stanford.edu/~candes/nesta/NESTA.pdf">Becker09</a>¬†S. Becker, J. Bobin, E. Candes. NESTA: A Fast and Accurate First-Order Method for¬†Sparse Recovery. Technical Report, Caltech, 2009</li><li><a href="http://www.princeton.edu/~sbubeck/Bubeck14.pdf">Bubeck14</a>¬†S. Bubeck, Theory of Convex Optimization for Machine Learning</li><li><a href="http://www.ece.rice.edu/~mam15/amp_pnas.pdf">Donoho09</a> D. Donoho, A. Maleki, A. Montanari. Message-passing algorithms for compressed sensing. Proceedings of National Academy of Sciences, 2009.</li><li><a href="http://arxiv.org/pdf/1311.0035v1.pdf">Mousavi13</a> A. Mousavi, A. Maleki, R. Baranuick. Parameterless optimal approximate message passing. CoRR, 2013</li><li>[Nesterov83] Y. Nesterov.¬†A method for solving a convex programming problem with convergence rate O(1/k2). Dokaldy AN SSR, 1983</li><li>[Nesterov88] Y. Nesterov.¬†On an approach to the construction of optimal methods of minimization of smooth convex functions. Ekonom. i. Mat. Metody, 1988</li><li>[Nesterov04] Y. Nesterov. Introductory Lectures On Convex Programming: A Basic Course. Kluwer Academic Publishers,¬†2004</li><li><a href="http://luthuli.cs.uiuc.edu/~daf/courses/Optimization/MRFpapers/[Nesterov05].pdf">Nesterov05</a> Y. Nesterov. Smooth minimization of non-smooth functions. Mathematical Programming, 2005.</li><li><a href="http://www.optimization-online.org/DB_FILE/2007/09/1784.pdf">Nesterov07</a> Y. Nesterov.¬†Gradient methods for minimizing composite objective function. Report, CORE, 2007</li><li><a href="http://galton.uchicago.edu/~lekheng/courses/31060s13/nesterov.pdf">Nesterov08</a> Y. Nesterov. How to advance in Structural Convex Optimization. Optima, 2008</li><li><a href="http://www.optimization-online.org/DB_FILE/2011/04/3004.pdf">Katya14</a> K. Scheinberg, D. Goldfarb, X. Bai. Fast First-Order Methods for Composite Convex Optimization with Backtracking. Foundations of Computational Mathematics, 2014.</li><li><a href="https://www.ipam.ucla.edu/publications/optut/optut_9300.pdf">Teboulle10</a>. M. Teboulle.¬†First-Order Methods for Optimization. IPAM Optimization Tutorials,¬†2010</li><li><a href="http://www.mit.edu/~dimitrib/PTseng/papers/apgm.pdf">Tseng08</a> P. Tseng. On Accelerated Proximal Gradient Algorithms for Convex-Concave Optimization. SIAM Journal of Optimization, 2008</li></ul><h2 id="implementations">Implementations</h2><ul><li><a href="http://web.stanford.edu/~boyd/papers/prox_algs/lasso.html">Stanford, LASSO</a></li><li><a href="http://www.caam.rice.edu/~optimization/disparse/LASSO/FISTA/pFistaLasso.html">Rice, LASSO-FISTA</a></li><li><a href="https://github.com/slipguru/l1l2py">Genoa, L1-L2 in Python</a></li><li><a href="http://www.eecs.berkeley.edu/~yang/software/l1benchmark/">UCB, L1 Benchmark</a></li><li><a href="http://www.ece.rice.edu/~tag7/Tom_Goldstein/CGIST.html">Rice, CGIST</a></li><li><a href="http://www.mit.edu/~dimitrib/PTseng/papers/apg_alg.m">MIT, Approx. Proximal Gradient</a></li><li><a href="http://www.math.ucla.edu/~wotaoyin/software.html">UCLA</a></li><li><a href="http://www.mathworks.com/matlabcentral/fileexchange/16204-toolbox-sparse-optmization/content/toolbox_optim/perform_fb.m">MATLAB, Optim Toolbox</a></li><li><a href="https://github.com/gpeyre/numerical-tours/tree/master/matlab/solutions">Numerical algorithms in MATLAB</a></li><li><a href="http://www.numerical-tours.com">Numerical Tours</a></li></ul><h2 id="credits">Credits</h2><p>Main image from <a href="https://lossfunctions.tumblr.com/image/126914340062">lossfunctions.tumblr</a>.</p></div><ul class="post__social"><li><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-facebook"></i></a></li><li><a href="https://twitter.com/intent/tweet?&text=Accelerated first-order methods for regularization+http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/+by+Umayr Hassan" target="_blank"><i class="fa fa-twitter"></i></a></li><li><a href="https://plus.google.com/share?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-google-plus"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&source=Accelerated first-order methods for regularization&summary=&url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://www.stumbleupon.com/badge/?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-stumbleupon"></i></a></li><li><a href="https://www.reddit.com/submit?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-reddit-alien"></i></a></li><li><a href="https://www.tumblr.com/share/link?url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-tumblr"></i></a></li><li><a href="https://www.pinterest.com/pin/create/link/?description=&media=http://localhost:4000/assets/images/agd.png&url=http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/" target="_blank"><i class="fa fa-pinterest"></i></a></li></ul></div></div><div class="section-padding--none"><div class="grid"><hr class="sep"/></div></div><div class="section-padding"><div class="grid-small"> <span class="post__author">Posted by <a href="http://umayrh.github.io" title="More By Umayr Hassan">Umayr Hassan</a></span><p class="post__bio"></p></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="http://localhost:4000/2016/12/23/a-fearful-sphere-whose-center-is-everywhere/"><figure class="related__img"> <img src="/assets/images/screen-shot-2016-12-16-at-10-28-14-pm.png" alt="A fearful sphere, whose center is everywhere"/></figure><div><h2 class="related__text">A fearful sphere, whose center is everywhere</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/10/06/obituaries-harold-kuhn-1925-2014/"><div><h2 class="related__text">Obituaries: Harold Kuhn (1925‚Äì2014)</h2></div></a></article><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/07/24/accelerated-first-order-methods-for-regularization/"><figure class="related__img"> <img src="/assets/images/agd.png" alt="Accelerated first-order methods for regularization"/></figure><div><h2 class="related__text">Accelerated first-order methods for regularization</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get a weekly email of posts I‚Äôve added to the site.</p><form method="POST" action="&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__tags"><li><a class="footer__link" href="/tag/mathematics">Mathematics</a></li><li><a class="footer__link" href="/tag/algorithms">Algorithms</a></li><li><a class="footer__link" href="/tag/convex-optimization">Convex Optimization</a></li><li><a class="footer__link" href="/tag/java">Java</a></li><li><a class="footer__link" href="/tag/obit">Obit</a></li></ul><ul class="footer__social"><li><a href="https://www.linkedin.com/in/umayr/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://github.com/umayrh" target="_blank"><i class="fa fa-github"></i></a></li><li><a href="https://gitlab.com/umayrh" target="_blank"><i class="fa fa-gitlab"></i></a></li></ul></div></div></footer><section class="contact popup"><div class="popup__close"><div class="popup__exit"></div></div><div class="contact__container popup__container"><div class="contact__img"><figure class="absolute-bg" style="background-image: url(/assets/images/form_contact.jpg);"></figure></div><div class="contact__content"><div class="contact__mast section-padding--half"><div class="grid"><h2>Contact</h2></div></div><div class="section-padding--none"><hr class="sep"/></div><div class="contact__form section-padding--half"><div class="grid-xlarge"> <form id="form" class="form" action="https://formcarry.com/s/UuffFC2ATgC" method="POST"><div class="form__subcontainer"><div> <label for="form-first-name">First Name</label> <input type="text" name="first-name" id="form-first-name" required></div><div> <label for="form-last-name">Last Name</label> <input type="text" name="last-name" id="form-last-name" required></div></div><div> <label for="form-email">E-Mail</label> <input type="email" name="email" id="form-email" required></div><div> <label for="form-message">Message</label> <textarea name="message" id="form-message" rows="3"></textarea></div><div class="form__submit"><div class="form__btn"> <input type="submit" value="Send"></div></div><p class="form__message"></p></form></div></div></div></div></section><script src="/assets/js/app.min.js"></script></body></html>
  