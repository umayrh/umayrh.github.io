<!DOCTYPE html><html lang="en"><meta charset="utf-8"><meta http-equiv="X-UA-Compatible" content="IE=edge"><meta name="viewport" content="width=device-width, initial-scale=1"><title>Nonnegative Matrix Factorization, 1 | Sketchy Polytopes</title><meta name="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="name" content="Umayr Hassan"><meta itemprop="description" content="Algorithms, optimization; systems, data, and people "><meta itemprop="image" content="http://localhost:4000/assets/images/basismap_k_3-gray.png"><meta property="og:url" content="http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/"><meta property="og:type" content="website"><meta property="og:title" content="Nonnegative Matrix Factorization, 1 | Sketchy Polytopes"><meta property="og:site_name" content="Sketchy Polytopes"><meta property="og:description" content="Algorithms, optimization; systems, data, and people "><meta property="og:image" content="http://localhost:4000/assets/images/basismap_k_3-gray.png"><meta name="twitter:url" content="http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/"><meta name="twitter:card" content="summary"><meta name="twitter:title" content="Nonnegative Matrix Factorization, 1 | Sketchy Polytopes"><meta name="twitter:site" content="Sketchy Polytopes"><meta name="twitter:description" content="Algorithms, optimization; systems, data, and people "><meta property="twitter:image" content="http://localhost:4000/assets/images/basismap_k_3-gray.png"><link rel="icon" type="image/x-icon" href="/assets/images/favicon.ico"><link rel="stylesheet" href="/assets/css/app.min.css"><link rel="alternate" type="application/rss+xml" title="Sketchy Polytopes" href="/feed.xml"><link rel="canonical" href="/2014/10/07/nonnegative-matrix-factorization-1/"> <script type="text/x-mathjax-config"> MathJax.Hub.Config({ tex2jax: { skipTags: ['script', 'noscript', 'style', 'textarea', 'pre'], inlineMath: [['$','$']] } }); </script> <script src="https://cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML" type="text/javascript"></script></head><body id="nonnegative-matrix-factorization-1" class="post-layout"><header class="header"> <a class="header__title" href="http://localhost:4000/">Sketchy Polytopes</a><nav><ul class="header__list"><li><a href="/">Stories</a></li><li><a href="/style-guide">Style Guide</a></li><li><span class="popup__open">Contact</span></li></ul></nav></header><main class="💈"><div class="post"><article itemprop="blogPost" itemscope itemtype="http://schema.org/BlogPosting"><div class="post__header section-padding--double"><div class="grid-small"><h2 itemprop="name headline">Nonnegative Matrix Factorization, 1</h2><time class="post__date" datetime="2014-10-07T00:00:00-07:00" itemprop="datePublished">7 Oct 2014</time></div></div><div class="post__img"><div><figure class="absolute-bg" style="background-image: url('/assets/images/basismap_k_3-gray.png');"></figure></div></div><div class="post__content section-padding"><div class="grid"><div id="markdown" itemprop="articleBody"><p>The aim of this post is to highlight the utility of non-negative factorization (NMF) in data analysis through examples. In a different post, we’ll talk theory and implementation by thinking of NMF as constrained optimization.</p><p><strong>Learning parts of a whole</strong></p><p>When Lee and Seung [Lee99] re-introduced non-negative matrix factorization (NMF), they emphasized on the algorithm’s ability to learn parts of a whole. That is, a good and interpretable low-rank approximation of a function or data e.g. features of a face or the semantic components of text. Moreover, in interpreting the algorithm as a neural network, they argue that the part-based learning is a consequence of the non-negativity constraint (neuron firing rates and synaptic strengths are non-negative). This is contrasted with well-known techniques such as Principle Component Analysis (<a href="http://en.wikipedia.org/wiki/Principal_component_analysis">PCA</a>) and Vector Quantization (<a href="http://en.wikipedia.org/wiki/Vector_quantization">VQ</a>), which learn by creating archetypes of objects. In all three cases, the algorithms model data as a linear and additive combination of “basis” functions or vectors - the difference lying in the  constraints imposed on the model.</p><p>Thus, given an $n \times m$ data matrix $V$, we seek matrix factors $W$ and $H$, which are $n \times k$ and $k \times m$, respectively such that some loss function (one below is for Frobenius norm)  is minimized for a given value of $k$:</p><table><tbody><tr><td>$\mathbf{V} \approx \mathbf{W} \mathbf{H} \ min_{W,H} \frac{1}{2}</td><td> </td><td>\mathbf{V} - \mathbf{W}\mathbf{H}</td><td> </td><td>_{F} \ s.t.\ \mathbf{W}, \mathbf{H} \ge 0$</td></tr></tbody></table><p>In some ways, the issues raised by Lee99 were not new. In chemometrics and environmetrics, NMF was already being used for analyzing molecular spectra Juvela94 and <a href="http://www.epa.gov/scram001/receptorindex.htm">receptor modeling</a>  [Hopke00]. This was motivated by the (1) lack of interpretation for factors produced by PCA (which, when addressed by matrix rotations can cause the problem to have non-unique solutions), and  (2) the sensitivity of PCA to data scaling. The latter issue is connected with the need to account for uncertainty in data, which is harder to achieve with PCA yet important for modeling in those areas:</p><p>$min_{W,H} \sum_i \sum_j \frac{v_{ij} - w_{ij} h_{ij}}{s_{ij}}$</p><p>Here’s R code for latent semantic indexing using the excellent <a href="http://cran.r-project.org/web/packages/NMF/index.html">NMF</a> package. The dataset used is AssociatedPress (from topicmodel package), which is available as a DocumentTermMatrix (see the <a href="http://www.cran.r-project.org/web/packages/tm/index.html">TM</a> package) encoding the <a href="http://en.wikipedia.org/wiki/Tf–idf">TF-IDF</a> of terms in various documents. NMF is calculated for a vector of four factors and then the terms closet to each factor are drawn as a word cloud (see the <a href="http://cran.r-project.org/web/packages/wordcloud/index.html">wordcloud</a> package).</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">library</span><span class="p">(</span><span class="n">topicmodels</span><span class="p">)</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">tm</span><span class="p">)</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">NMF</span><span class="p">)</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">wordcloud</span><span class="p">)</span><span class="w">

</span><span class="n">data</span><span class="p">(</span><span class="n">AssociatedPress</span><span class="p">)</span><span class="w"> </span><span class="n">dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">AssociatedPress</span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">20</span><span class="p">,]</span><span class="w"> </span><span class="n">dtm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">removeSparseTerms</span><span class="p">(</span><span class="n">dtm</span><span class="p">,</span><span class="w"> </span><span class="m">0.9</span><span class="p">)</span><span class="w"> </span><span class="n">dm</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">as.matrix</span><span class="p">(</span><span class="n">dtm</span><span class="p">)</span><span class="w"> </span><span class="nf">dim</span><span class="p">(</span><span class="n">dm</span><span class="p">)</span><span class="w">

</span><span class="n">k</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="m">4</span><span class="w"> </span><span class="n">nmf.res</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">dm</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="s2">"snmf/r"</span><span class="p">)</span><span class="w"> </span><span class="n">basismap</span><span class="p">(</span><span class="n">nmf.res</span><span class="p">)</span><span class="w"> </span><span class="n">coefmap</span><span class="p">(</span><span class="n">nmf.res</span><span class="p">)</span><span class="w">

</span><span class="c1"># top basis and coefficients res.coef &lt;- coef(nmf.res) res.bas &lt;- basis(nmf.res)</span><span class="w">

</span><span class="n">set.seed</span><span class="p">(</span><span class="m">1234</span><span class="p">)</span><span class="w"> </span><span class="k">for</span><span class="w"> </span><span class="p">(</span><span class="n">n</span><span class="w"> </span><span class="k">in</span><span class="w"> </span><span class="n">seq</span><span class="p">(</span><span class="m">1</span><span class="p">,</span><span class="w"> </span><span class="n">k</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="p">))</span><span class="w"> </span><span class="p">{</span><span class="w"> </span><span class="n">freq</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">res.coef</span><span class="p">[</span><span class="n">n</span><span class="p">,(</span><span class="n">order</span><span class="p">(</span><span class="n">res.coefn</span><span class="p">,],</span><span class="w"> </span><span class="n">decreasing</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="kc">TRUE</span><span class="p">)</span><span class="w"> </span><span class="p">[</span><span class="m">1</span><span class="o">:</span><span class="m">40</span><span class="p">)]</span><span class="w"> </span><span class="n">wordcloud</span><span class="p">(</span><span class="nf">names</span><span class="p">(</span><span class="n">freq</span><span class="p">),</span><span class="w"> </span><span class="nf">round</span><span class="p">(</span><span class="n">freq</span><span class="p">),</span><span class="w"> </span><span class="n">scale</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="m">0.1</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">),</span><span class="w"> </span><span class="n">colors</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="n">brewer.pal</span><span class="p">(</span><span class="m">6</span><span class="p">,</span><span class="w"> </span><span class="s2">"Dark2"</span><span class="p">))</span><span class="w"> </span><span class="n">print</span><span class="p">(</span><span class="n">freq</span><span class="p">)</span><span class="w"> </span><span class="n">print</span><span class="p">(</span><span class="s2">"=========================="</span><span class="p">)</span><span class="w"> </span><span class="p">}</span><span class="w">
</span></code></pre></div></div><p><a href="https://umayrh.files.wordpress.com/2014/09/nmf6_lsi_ap_k4_wc.png"><img src="https://umayrh.files.wordpress.com/2014/09/nmf6_lsi_ap_k4_wc.png?w=660" alt="nmf6_lsi_ap_k4_wc" /></a></p><p><em>Summary</em>: NMF is useful because some applications (e.g. in physical sciences and signal processing) naturally restrict the data and hence latent factor to be non-negative, and NMF can satisfy this constraint.</p><p><strong>Collaborative filtering</strong></p><p>While NMF steadily found new applications in diverse fields such as signal processing and data mining [Cichoki09], perhaps matrix factorization’s role in solving the <a href="http://en.wikipedia.org/wiki/Netflix_Prize">Netflix prize</a> problem  [Koren09, or this <a href="http://blog.echen.me/2011/10/24/winning-the-netflix-prize-a-summary/">summary</a> made it more prominent within machine learning communities. Koren09 starts out with distinguishing two types of learning models for <a href="http://en.wikipedia.org/wiki/Collaborative_filtering">collaborative filtering</a>: neighborhood and latent factor. Neighborhood models find similarity between users or between items, whereas latent factor models find variables that can explain both user preferences and item characteristics. Within latent factor models, the authors focus on  <a href="http://en.wikipedia.org/wiki/Singular_value_decomposition">SVD</a> (instead of NMF, perhaps because, unlike physical science, negative ratings are not much of a concern here). Classical SVD’s disadvantage seems to lies in not being able to handle missing entries in the data (user-item matrix). Drawing on previous work, they argue for the need to (1) fit model based on known data value alone, and (2) avoid overfitting using regularization.</p><p>The basic model, then, according to Koren09 is the following optimization problem:</p><table><tbody><tr><td>$min_{W,H} \sum_{(i,j) \in K} (v_{ij} - w_{i}^{T} h_{j})^2 + \lambda (</td><td> </td><td>w_i</td><td> </td><td>^2 +</td><td> </td><td>h_j</td><td> </td><td>^2) = $</td></tr></tbody></table><p>where $v_{ij}$ is the rating for item $j$ by user $i$, $w_i$ is the user’s factor vector, $h_j$ the item’s factor vector, and λ is the regularization parameter.</p><p>The paper also highlights two interesting extensions to the basic model:</p><ul><li>Biases: $\hat{v}<em>{ij} = \mu + b_i + b_j + w</em>{i}^{T}h_{j}$</li><li>Temporal dynamics: $\hat{v}<em>{ij}(t) = \mu + b</em>{i}(t) + b_{j}(t) + w_{i}^{T}h_{j}$</li></ul><p>The Netflix prize is also a <a href="http://en.wikipedia.org/wiki/Matrix_completion">Matrix Completion</a> problem that we can solve using NMF (though factorizing first might be more inefficient). Here’s R code for collaborative filtering based on the <a href="http://grouplens.org/datasets/movielens/">MovieLens</a> 100K dataset (“100,000 ratings from 1000 users on 1700 movies”). Much of the code is involved with reading and parsing the MovieLens data and metadata, and draws on <a href="http://ggplot2.org">ggplot2</a> and <a href="http://directlabels.r-forge.r-project.org">directlabels</a> for visualization.</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="c1"># read the Movie Lens data from file u1.base &lt;- read.table(file="~/Desktop/mlearn/Rcode/ml-100k/u1.base",sep='\t',header=F, col.names=c("userId", "itemId", "rating", "timestamp"), colClasses = c("integer", "integer", "integer", "integer"))</span><span class="w">

</span><span class="err">\</span><span class="c1"># get all unique users and items unq.users &lt;- sort(unique(u1.base$userId)) unq.items &lt;- sort(unique(u1.base$itemId)) library(bit64) unq.users.i64 &lt;- as.integer64(unq.users) unq.items.i64 &lt;- as.integer64(unq.items)</span><span class="w">

</span><span class="err">\</span><span class="c1"># hashmaps store the index of each user and item users.hm &lt;-hashmap.integer64(unq.users.i64, nunique = length(unq.users.i64)) items.hm &lt;-hashmap.integer64(unq.items.i64, nunique = length(unq.items.i64))</span><span class="w">

</span><span class="err">\</span><span class="c1"># populate the ratings matrix u1.base.mat &lt;- matrix(data = 0, nrow = length(unq.users.i64), ncol = length(unq.items.i64)) for (i in 1:nrow(u1.base)) { u1.base.mathashpos(users.hm, u1.basei,1), hashpos(items.hm, u1.basei,2) &lt;- u1.basei,3 } rownames(u1.base.mat) &lt;- unq.users colnames(u1.base.mat) &lt;- unq.items</span><span class="w">

</span><span class="err">\</span><span class="c1"># factorize, with as many factors as movie genres given in the ML data library(NMF) k &lt;- 19 set.seed(8888) nmf.res &lt;- nmf(u1.base.mat, nrun = 5, rank = k, method = "snmf/r") basismap(nmf.res) coefmap(nmf.res)</span><span class="w">

</span><span class="err">\</span><span class="c1"># top basis and coefficients, and metadata res.coef &lt;- coef(nmf.res) res.bas &lt;- basis(nmf.res) item.col.names = c("movieId", "movieTitle", "releaseDate", "vidReleaseDate", "imdbUrl", "unknown", "Action", "Adventure", "Animation", "Children", "Comedy", "Crime", "Documentary", "Drama", "Fantasy", "FilmNoir", "Horror", "Musical", "Mystery", "Romance", "SciFi", "Thriller", "War", "Western") item.col.classes = c("integer", "character", "character", "character", "character", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer", "integer") # read.table does not work without "latin1" encoding and disabling quotes u.item.txt &lt;- readLines("~/Desktop/mlearn/Rcode/ml-100k/u.item", encoding = "latin1") u.items &lt;- read.table(text=u.item.txt,sep='|',header=F, col.names=item.col.names, colClasses = item.col.classes, encoding = "latin1", quote = "") u.items1, u.itemsorder(res.coef1,, decreasing = TRUE) 1:20, 2</span><span class="w">

</span><span class="err">\</span><span class="c1"># scatterplot: one factor against another (as in Koren09) factor1.idx &lt;- order(res.coef1,, decreasing = TRUE) 1:20 factor2.idx &lt;- order(res.coef2,, decreasing = TRUE) 1:20 plot.idx &lt;- union(factor1.idx, factor2.idx) qplot.dat &lt;- data.frame(factor1 = res.coef1, plot.idx, factor2 = res.coef2, plot.idx, labels = u.itemsplot.idx,2) library(ggplot2) library(directlabels) plot.obj &lt;-qplot(factor1, factor2, data=qplot.dat, colour = labels, main="Factor1 vs Factor2") direct.label(plot.obj)</span><span class="w">

</span><span class="err">\</span><span class="c1"># plot hierarchical clusters for two factors u.item.factor &lt;- t(res.coef) rownames(u.item.factor) &lt;- as.list(u.items1:nrow(u.item.factor),2) u.item.factor.sampled &lt;- u.item.factorplot.idx,1:2 u.item.factor.dist = dist(u.item.factor.sampled, method = "manhattan") u.item.factor.clust = hclust(u.item.factor.dist, method = "ward") plot(u.item.factor.clust) /code</span><span class="w">
</span></code></pre></div></div><p>The scatterplot and dendogram outputs of the code:</p><p><a href="https://umayrh.files.wordpress.com/2014/10/nmf7_cf_ml_k19_hclust.png"><img src="http://umayrh.files.wordpress.com/2014/10/nmf7_cf_ml_k19_qplot.png?w=300" alt="nmf7_cf_ml_k19_qplot" /><img src="http://umayrh.files.wordpress.com/2014/10/nmf7_cf_ml_k19_hclust.png?w=300" alt="nmf7_cf_ml_k19_hclust" /></a></p><p><em>Caveat</em>: while NMF itself can be used for matrix completion, the NMF package in R cannot since it cannot account for missing values. An alternative is to use <a href="http://cran.r-project.org/web/packages/softImpute/">softImpute</a>.</p><p><em>Summary</em>: another reason for the success of NMF is the ability to extend the basic model for use in specific applications (like collaborative filtering).</p><p><strong>Cluster analysis</strong></p><p>Non-negative matrix factorization is essentially a form of cluster analysis - the critical difference being that the clusters are derived from linear, additive latent factors instead of item-to-item similarity/distance metrics/divergences. So, it’s not surprising that researchers were interested in the connections between traditional unsupervised learning methods like K-means and NMF. Two interesting papers in this line are Kim07 and Kim08. Apart from analyzing the relationship between K-mean and NMF, they highlight (though, not introduce):</p><ul><li>the importance of sparsity,</li><li>utility of <em>purity</em> and <em>entropy</em> metrics to evaluate clustering quality, and</li><li>the utility of of <em>consensus maps</em> Burnet04 for model selection</li></ul><p>NMF, contrary to Lee and Seung’s intuition and experiments, doesn’t always produce a part-based representation. In this, Kim07 and Kim08 followed the arguments and counter-examples of Li01 and Hoyer04. Since, as Hoyer04 states, ”the sparseness given by NMF is somewhat of a side-effect rather than a goal,” regularization needs to be explicitly introduced to control the degree of sparseness. Li07 does that by penalizing the energy of the factors in the optimization objective (similar to the Koren09 basic model); Hoyer04 introduces sparseness constraints separately on each factor; while Kim07, Kim08 mix L1 and L2 norms to impose sparsity on one of the factors. Sparseness may not only improve the quality of the result but, as Kim08 show, also result in better execution time. The two algorithms, SNMF/R and SNMF/L, introduced in Kim07 are available in the R <a href="http://cran.r-project.org/web/packages/NMF/index.html">NMF</a> package.</p><p>Purity and entropy are two traditional <a href="http://nlp.stanford.edu/IR-book/html/htmledition/evaluation-of-clustering-1.html">clustering metrics</a>. Purity measures the accuracy of assignment (n data point, k clusters and l classes) - larger value implies better clustering. Entropy measures the amount of information-theoretic order - smaller value is better.</p><table><tbody><tr><td>$purity = \frac{1}{n}\sum_{k}\max_{l}</td><td>c_k \cap C_l</td><td>$</td></tr></tbody></table><table><tbody><tr><td>$entropy = \frac{-1}{n\log_{2}l} \sum_{k} \sum_{l}</td><td>c_k \cap C_l</td><td>\log_{2}\frac{</td><td>c_k \cap C_l</td><td>}{c_k}$</td></tr></tbody></table><p>For model selection (i.e. the number of factors to use), Burnet04 introduced <em>consensus matrix</em> and the related <em>dispersion coefficient</em>. Since NMF may not converge to the same solution on each run, consensus matrix (C) encodes the probability of assignment agreement over multiple runs. The dispersion coefficient is calculated as the “Pearson correlation of two distance matrices: the first, I-C , is the distance between samples induced by the consensus matrix, and the second is the distance between samples induced by the linkage used in the reordering of C.” Kim07 described a more comprehensible version of dispersion coefficient that only uses the consensus matrix. It’s value also lies between 0 and 1, and the higher the better (1 indicates complete consensus). The NMF package implements this version.</p><p>$dispersion = \frac{1}{n^2}\sum_{i}\sum_{j}4(C_{ij} - \frac{1}{2})^2$</p><p>Here’s R code to analyze the <a href="https://archive.ics.uci.edu/ml/datasets/Iris">iris dataset</a> using the <a href="http://cran.r-project.org/web/packages/NMF/index.html">NMF</a> package:</p><div class="language-R highlighter-rouge"><div class="highlight"><pre class="highlight"><code><span class="n">data</span><span class="p">(</span><span class="n">iris</span><span class="p">)</span><span class="w"> </span><span class="n">library</span><span class="p">(</span><span class="n">NMF</span><span class="p">)</span><span class="w"> </span><span class="n">ir</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">t</span><span class="p">(</span><span class="n">as.matrix</span><span class="p">(</span><span class="n">iris</span><span class="p">,</span><span class="w"> </span><span class="m">1</span><span class="o">:</span><span class="m">4</span><span class="p">));</span><span class="w">

</span><span class="n">ir.res.k2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">ir</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">nrun</span><span class="w"> </span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">12345</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lee"</span><span class="p">)</span><span class="w"> </span><span class="n">ir.res.k3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">ir</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">nrun</span><span class="w"> </span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">12345</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lee"</span><span class="p">)</span><span class="w"> </span><span class="n">ir.res.k4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">ir</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">nrun</span><span class="w"> </span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">12345</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"lee"</span><span class="p">)</span><span class="w">

</span><span class="n">p.k2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">purity</span><span class="p">(</span><span class="n">ir.res.k2</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="n">p.k3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">purity</span><span class="p">(</span><span class="n">ir.res.k3</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="n">p.k4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">purity</span><span class="p">(</span><span class="n">ir.res.k4</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="m">2</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">p.k2</span><span class="p">,</span><span class="w"> </span><span class="n">p.k3</span><span class="p">,</span><span class="w"> </span><span class="n">p.k4</span><span class="p">),</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"factors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"purity"</span><span class="p">)</span><span class="w">

</span><span class="n">e.k2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">entropy</span><span class="p">(</span><span class="n">ir.res.k2</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="n">e.k3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">entropy</span><span class="p">(</span><span class="n">ir.res.k3</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="n">e.k4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">entropy</span><span class="p">(</span><span class="n">ir.res.k4</span><span class="p">,</span><span class="w"> </span><span class="n">iris</span><span class="p">,</span><span class="m">5</span><span class="p">)</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="m">2</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">e.k2</span><span class="p">,</span><span class="w"> </span><span class="n">e.k3</span><span class="p">,</span><span class="w"> </span><span class="n">e.k4</span><span class="p">),</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"factors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"entropy"</span><span class="p">)</span><span class="w">

</span><span class="n">par</span><span class="p">(</span><span class="n">mfrow</span><span class="o">=</span><span class="nf">c</span><span class="p">(</span><span class="m">2</span><span class="p">,</span><span class="m">2</span><span class="p">))</span><span class="w"> </span><span class="n">consensusmap</span><span class="p">(</span><span class="n">ir.res.k2</span><span class="p">)</span><span class="w"> </span><span class="n">consensusmap</span><span class="p">(</span><span class="n">ir.res.k3</span><span class="p">)</span><span class="w"> </span><span class="n">consensusmap</span><span class="p">(</span><span class="n">ir.res.k4</span><span class="p">)</span><span class="w">

</span><span class="n">d.k2</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dispersion</span><span class="p">(</span><span class="n">ir.res.k2</span><span class="p">)</span><span class="w"> </span><span class="n">d.k3</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dispersion</span><span class="p">(</span><span class="n">ir.res.k3</span><span class="p">)</span><span class="w"> </span><span class="n">d.k4</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dispersion</span><span class="p">(</span><span class="n">ir.res.k4</span><span class="p">)</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="m">2</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">d.k2</span><span class="p">,</span><span class="w"> </span><span class="n">d.k3</span><span class="p">,</span><span class="w"> </span><span class="n">d.k4</span><span class="p">),</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"factors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dispersion"</span><span class="p">)</span><span class="w">

</span><span class="n">ir.res.k2.s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">ir</span><span class="p">,</span><span class="w"> </span><span class="m">2</span><span class="p">,</span><span class="w"> </span><span class="n">nrun</span><span class="w"> </span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">12345</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"snmf/r"</span><span class="p">)</span><span class="w"> </span><span class="n">ir.res.k3.s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">ir</span><span class="p">,</span><span class="w"> </span><span class="m">3</span><span class="p">,</span><span class="w"> </span><span class="n">nrun</span><span class="w"> </span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">12345</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"snmf/r"</span><span class="p">)</span><span class="w"> </span><span class="n">ir.res.k4.s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">nmf</span><span class="p">(</span><span class="n">ir</span><span class="p">,</span><span class="w"> </span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="n">nrun</span><span class="w"> </span><span class="o">=</span><span class="m">10</span><span class="p">,</span><span class="w"> </span><span class="n">seed</span><span class="o">=</span><span class="m">12345</span><span class="p">,</span><span class="w"> </span><span class="n">method</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"snmf/r"</span><span class="p">)</span><span class="w">

</span><span class="n">d.k2.s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dispersion</span><span class="p">(</span><span class="n">ir.res.k2.s</span><span class="p">)</span><span class="w"> </span><span class="n">d.k3.s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dispersion</span><span class="p">(</span><span class="n">ir.res.k3.s</span><span class="p">)</span><span class="w"> </span><span class="n">d.k4.s</span><span class="w"> </span><span class="o">&lt;-</span><span class="w"> </span><span class="n">dispersion</span><span class="p">(</span><span class="n">ir.res.k4.s</span><span class="p">)</span><span class="w"> </span><span class="n">plot</span><span class="p">(</span><span class="m">2</span><span class="o">:</span><span class="m">4</span><span class="p">,</span><span class="w"> </span><span class="nf">c</span><span class="p">(</span><span class="n">d.k2.s</span><span class="p">,</span><span class="w"> </span><span class="n">d.k3.s</span><span class="p">,</span><span class="w"> </span><span class="n">d.k4.s</span><span class="p">),</span><span class="w"> </span><span class="n">xlab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"factors"</span><span class="p">,</span><span class="w"> </span><span class="n">ylab</span><span class="w"> </span><span class="o">=</span><span class="w"> </span><span class="s2">"dispersion (snmf/r)"</span><span class="p">)</span><span class="w"> </span><span class="o">/</span><span class="n">code</span><span class="w">
</span></code></pre></div></div><p>This code uses the package’s heatmap tools: <code class="highlighter-rouge">basismap</code>, <code class="highlighter-rouge">coefmap</code> and <code class="highlighter-rouge">consensusmap</code>. If samples are taken to be the rows of a $n_m$ data matrix $V$ and features are taken to be its columns, then <code class="highlighter-rouge">basismap</code> draws the $n_k$ matrix factor W, which represents the contribution of each factor in each sample. Similarly, <code class="highlighter-rouge">coefmap</code> helps visualize the $k_m$ matrix factor H, which represents the contribution of each feature in each factor. Here are the rank-2 and rank-3 maps:</p><p><a href="https://umayrh.files.wordpress.com/2014/09/k_3.png"><img src="http://umayrh.files.wordpress.com/2014/09/k_3.png?w=300" alt="k_3" /></a> Basis and coefficient maps for k = 3/caption <a href="https://umayrh.files.wordpress.com/2014/09/k_2.png"><img src="http://umayrh.files.wordpress.com/2014/09/k_2.png?w=300" alt="Basis and coefficient maps for k = 2" /></a> Basis and coefficient maps for k = 2/caption</p><p>Not surprisingly, purity is maximized at k = 3 while entropy is minimized. Surprisingly, though, dispersion decreases as factors increase (also obvious from the consensusmap plots). Though Kim07, Kim08 don’t seem to discuss this, it may be that dispersion on its own is not reliable for model selection.</p><p><a href="https://umayrh.files.wordpress.com/2014/10/nmf3_iris_k2_4_entropy.png"><img src="https://umayrh.files.wordpress.com/2014/10/nmf3_iris_k2_4_entropy.png?w=300" alt="nmf3_iris_k2_4_entropy" /> <img src="https://umayrh.files.wordpress.com/2014/10/nmf3_iris_k2_4_purity.png?w=300" alt="nmf3_iris_k2_4_purity" /></a> <a href="https://umayrh.files.wordpress.com/2014/10/nmf3_iris_k2_4_sm.png"><img src="https://umayrh.files.wordpress.com/2014/10/nmf3_iris_k2_4_sm.png?w=300" alt="nmf3_iris_k2_4_sm" /></a></p><p><em>Summary</em>: NMF lends well to cluster analysis though model selection seems tricky.</p><p><strong>References</strong></p><ul><li><a href="http://arxiv.org/pdf/1111.0952v1.pdf">Arora12a</a> S. Arora, R. Ge, R. Kannan, and A. Moitra. Computing a Nonnegative Matrix Factorization – Provably. STOC, 2012</li><li><a href="http://arxiv.org/abs/1204.1956">Arora12b</a> S. Arora, R. Ge, A. Moitra. Learning Topic Models - Going Beyond SVD. FOCS, 2012</li><li><a href="http://www.pnas.org/content/101/12/4164.full.pdf+html">Brunet04</a> J. Brunet, P. Tamayo, T. Golub, and J. Mesirov. Metagenes and molecular pattern discovery using matrix factorization. PNAS, 2004.</li><li><a href="http://www.wiley.com/WileyCDA/WileyTitle/productCd-0470746661.html">Cichoki09</a> A. Cichoki, R. Zdunek, A. H. Phan, S. Ammari. Nonnegative Matrix and Tensor Factorizations: Applications to exploratory multi-way data analysis and blind source separation, 2009.</li><li><a href="https://web.stanford.edu/~vcs/papers/NMFCDP.pdf">Donoho03</a> D. Donoho and V. Stodden. When Does Non-Negative Matrix Factorization Give a Correct Decomposition into Parts? NIPS, 2003.</li><li><a href="http://www.epa.gov/ttnamti1/files/ambient/pm25/workshop/laymen.pdf">Hopke00</a> P. K. Hopke. A guide to positive matrix factorization.Workshop on UNMIX and PMF as Applied to PM2, 2000.</li><li><a href="http://jmlr.org/papers/volume5/hoyer04a/hoyer04a.pdf">Hoyer04</a> P. O. Hoyer. Non-negative matrix factorization with sparseness constraints. JMLR, 2004.</li><li><a href="http://www.mrao.cam.ac.uk/yerac/juvela/juvela.html">Juvela94</a> M. Juvela, K. Lehtinen, and P. Paatero. The use of positive matrix factorization in the analysis of molecular line spectra from the thumbprint nebula.</li><li>D. P. Clemens and R. Barvainis, eds., <em>Clouds, Cores, and Low Mass Stars</em>, volume 65 of <em>ASP Conference Series</em>, 1994.</li><li><a href="http://adsabs.harvard.edu/abs/1996MNRAS.280..616J">Juvela96</a> M. Juvela, K. Lehtinen, and P. Paatero. The use of positive matrix factorization in the analysis of molecular line spectra. <em>MNRAS</em>, 1996.</li><li><a href="http://bioinformatics.oxfordjournals.org/content/23/12/1495.full.pdf">Kim07</a> H. Kim and H. Park. Sparse non-negative matrix factorizations via alternating non-negativity-constrained least squares for microarray data analysis. Bioinformatics, 2007.</li><li><a href="http://www.cc.gatech.edu/~hpark/papers/GT-CSE-08-01.pdf">Kim08</a> J. Kim and H. Park. Sparse Nonnegative Matrix Factorization for Clustering. Georgia Tech Technical Report, GT-CSE-08-01, 2008.</li><li><a href="https://datajobs.com/data-science-repo/Recommender-Systems-%5BNetflix%5D.pdf">Koren09</a> Y. Koren, R. Bell, and C. Volinsky. Matrix factorization for recommender systems. IEEE Computer Society, 2009.</li><li><a href="research.yahoo.com/files/korenBellChapterSpringer.pdf">Koren11</a> Y. Koren, and R. Bell. Advances in collaborative filtering. Recommender Systems Handbook, 2011.</li><li><a href="http://hebb.mit.edu/people/seung/papers/ls-lponm-99.pdf">Lee99</a> D. D. Lee, and H. S. Seung. Learning the parts of objects by non-negative matrix factorization. Nature, 1999.</li><li><a href="http://hebb.mit.edu/people/seung/papers/nmfconverge.pdf">Lee01</a> D. D. Lee and H. S. Seung. Algorithms for non-negative matrix factorization. NIPS, 2001.</li><li><a href="http://www.ai.mit.edu/courses/6.899/papers/3B_04.PDF">Li01</a> S. Z. Li, X. Hou, H. Zhang, and Q. Cheng. Learning spatially localized parts-based representations. Proc. IEEE Conf. on Computer Vision and Pattern Recognition (CVPR), 2001.</li><li><a href="http://www.cs.berkeley.edu/~jordan/papers/li-ding-jordan-icdm07.pdf">Li07</a> T. Li, C. Ding, M. I. Jordan. Solving Consensus and Semi-supervised Clustering Problems Using Nonnegative Matrix Factorization. ICDM, 2007.</li></ul></div><ul class="post__social"><li><a href="https://www.facebook.com/sharer/sharer.php?u=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-facebook"></i></a></li><li><a href="https://twitter.com/intent/tweet?&text=Nonnegative Matrix Factorization, 1+http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/+by+Umayr Hassan" target="_blank"><i class="fa fa-twitter"></i></a></li><li><a href="https://plus.google.com/share?url=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-google-plus"></i></a></li><li><a href="https://www.linkedin.com/shareArticle?mini=true&source=Nonnegative Matrix Factorization, 1&summary=&url=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://www.stumbleupon.com/badge/?url=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-stumbleupon"></i></a></li><li><a href="https://www.reddit.com/submit?url=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-reddit-alien"></i></a></li><li><a href="https://www.tumblr.com/share/link?url=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-tumblr"></i></a></li><li><a href="https://www.pinterest.com/pin/create/link/?description=&media=http://localhost:4000/assets/images/basismap_k_3-gray.png&url=http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/" target="_blank"><i class="fa fa-pinterest"></i></a></li></ul></div></div><div class="section-padding--none"><div class="grid"><hr class="sep"/></div></div><div class="section-padding"><div class="grid-small"> <span class="post__author">Posted by <a href="http://umayrh.github.io" title="More By Umayr Hassan">Umayr Hassan</a></span><p class="post__bio"></p></div></div></article></div><section class="related section-padding"><div class="grid-xlarge"><h2 class="related__title">Related</h2><div class="related__container"><article class="related__post"> <a class="related__link" href="http://localhost:4000/2014/10/07/nonnegative-matrix-factorization-1/"><figure class="related__img"> <img src="/assets/images/basismap_k_3-gray.png" alt="Nonnegative Matrix Factorization, 1"/></figure><div><h2 class="related__text">Nonnegative Matrix Factorization, 1</h2></div></a></article></div></div></section></main><footer class="footer section-padding"><div class="grid"><div class="subscribe" id="subscribe"><div class="subscribe__container"> <span class="subscribe__title">Subscribe</span><p class="subscribe__text">Get a weekly email of posts I’ve added to the site.</p><form method="POST" action="&amp;c=?" id="mc-signup" name="mc-embedded-subscribe-form" novalidate><div style="position: absolute; left: -5000px;" aria-hidden="true"> <input type="text" name="" tabindex="-1" value=""></div><div class="form-group"> <input id="mce-EMAIL" type="email" name="EMAIL" placeholder="Email Address"></div><div class="form__btn"> <input id="mc-submit" type="submit" value="Sign Up" name="subscribe"></div></form><p class="subscribe__error hidden"></p></div></div><hr class="sep--white"/><div class="footer__container"><ul class="footer__tags"><li><a class="footer__link" href="/tag/mathematics">Mathematics</a></li><li><a class="footer__link" href="/tag/algorithms">Algorithms</a></li><li><a class="footer__link" href="/tag/java">Java</a></li><li><a class="footer__link" href="/tag/obit">Obit</a></li><li><a class="footer__link" href="/tag/linear-algebra">Linear Algebra</a></li></ul><ul class="footer__social"><li><a href="https://www.linkedin.com/in/umayr/" target="_blank"><i class="fa fa-linkedin"></i></a></li><li><a href="https://github.com/umayrh" target="_blank"><i class="fa fa-github"></i></a></li><li><a href="https://gitlab.com/umayrh" target="_blank"><i class="fa fa-gitlab"></i></a></li></ul></div></div></footer><section class="contact popup"><div class="popup__close"><div class="popup__exit"></div></div><div class="contact__container popup__container"><div class="contact__img"><figure class="absolute-bg" style="background-image: url(/assets/images/form_contact.jpg);"></figure></div><div class="contact__content"><div class="contact__mast section-padding--half"><div class="grid"><h2>Contact</h2></div></div><div class="section-padding--none"><hr class="sep"/></div><div class="contact__form section-padding--half"><div class="grid-xlarge"> <form id="form" class="form" action="https://formcarry.com/s/UuffFC2ATgC" method="POST"><div class="form__subcontainer"><div> <label for="form-first-name">First Name</label> <input type="text" name="first-name" id="form-first-name" required></div><div> <label for="form-last-name">Last Name</label> <input type="text" name="last-name" id="form-last-name" required></div></div><div> <label for="form-email">E-Mail</label> <input type="email" name="email" id="form-email" required></div><div> <label for="form-message">Message</label> <textarea name="message" id="form-message" rows="3"></textarea></div><div class="form__submit"><div class="form__btn"> <input type="submit" value="Send"></div></div><p class="form__message"></p></form></div></div></div></div></section><script src="/assets/js/app.min.js"></script></body></html>
  